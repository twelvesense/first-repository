{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExplNode06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhjBeiaZKkUksjOxvV0BSR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twelvesense/first-repository/blob/master/ExplNode06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트: 멋진 작사가 만들기"
      ],
      "metadata": {
        "id": "H7Y58OA38ag3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 개요"
      ],
      "metadata": {
        "id": "ZNi-HieyNthJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1. 목표\n",
        "- LSTM를 이용하여, 입력이 한 단어일 때, 출력으로 문장을 만드는 NN 구현한다.\n",
        "- 노래 가사 작사 ???????????\n",
        "- 성능개선을 위한 분석한다."
      ],
      "metadata": {
        "id": "mYzHL5k9PUiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. 방법\n",
        "- Keras의 LSTM와 과제에서 주어진 coupus와 code를 이용하여 구현한다.\n",
        "-  1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있습니다.\n",
        "- tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환한다.\n",
        "- tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환한다.\n",
        "- 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠. 15로 제한\n",
        "- 성능평가 ?"
      ],
      "metadata": {
        "id": "NqYeus28PV8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 데이터 준비 "
      ],
      "metadata": {
        "id": "QqzFKCv3Aqjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. 기본 라이브러리 load하기"
      ],
      "metadata": {
        "id": "nrb5ndJ9Ao-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "# import pandas as pd\n",
        "import numpy as np\n",
        "import os, re\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "e4zFUXC4Dy31"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. 기본 code load하기"
      ],
      "metadata": {
        "id": "9u1az-JmwcHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    maxlen = int(max_tokens)     \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=maxlen)  "
      ],
      "metadata": {
        "id": "PCjJDaptw4zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2. 특수문자 양쪽에 공백을 넣고\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #  4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "    sentence = sentence.strip() # 5. 다시 양쪽 공백을 지웁니다\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "    return sentence\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=7000,               # 7000단어를 기억할 수 있는 tokenizer\n",
        "        filters=' ',                  # 앞에서 preprocess하므로, 여기서는 불필요\n",
        "        oov_token=\"<unk>\"             # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿈\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)  \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "\n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence를 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated  "
      ],
      "metadata": {
        "id": "1X8stFCCwmft"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-3. corpus 파일 download하기"
      ],
      "metadata": {
        "id": "o1udTHDCAx9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/lyrics\n",
        "!mkdir -p /content/lyrics\n",
        "!wget -O /content/lyrics.zip https://raw.githubusercontent.com/twelvesense/first-repository/master/data/lyrics/lyrics.zip\n",
        "!unzip /content/lyrics.zip -d /content/lyrics\n",
        "!rm /content/lyrics.zip"
      ],
      "metadata": {
        "id": "gpoAcRyWA-9P",
        "outputId": "0fb4419c-02c4-418c-a38b-14fe146a1b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-19 11:32:32--  https://raw.githubusercontent.com/twelvesense/first-repository/master/data/lyrics/lyrics.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2004593 (1.9M) [application/zip]\n",
            "Saving to: ‘/content/lyrics.zip’\n",
            "\n",
            "/content/lyrics.zip 100%[===================>]   1.91M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-05-19 11:32:32 (32.9 MB/s) - ‘/content/lyrics.zip’ saved [2004593/2004593]\n",
            "\n",
            "Archive:  /content/lyrics.zip\n",
            "  inflating: /content/lyrics/adele.txt  \n",
            "  inflating: /content/lyrics/al-green.txt  \n",
            "  inflating: /content/lyrics/alicia-keys.txt  \n",
            "  inflating: /content/lyrics/amy-winehouse.txt  \n",
            "  inflating: /content/lyrics/beatles.txt  \n",
            "  inflating: /content/lyrics/bieber.txt  \n",
            "  inflating: /content/lyrics/bjork.txt  \n",
            "  inflating: /content/lyrics/blink-182.txt  \n",
            "  inflating: /content/lyrics/bob-dylan.txt  \n",
            "  inflating: /content/lyrics/bob-marley.txt  \n",
            "  inflating: /content/lyrics/britney-spears.txt  \n",
            "  inflating: /content/lyrics/bruce-springsteen.txt  \n",
            "  inflating: /content/lyrics/bruno-mars.txt  \n",
            "  inflating: /content/lyrics/cake.txt  \n",
            "  inflating: /content/lyrics/dickinson.txt  \n",
            "  inflating: /content/lyrics/disney.txt  \n",
            "  inflating: /content/lyrics/dj-khaled.txt  \n",
            "  inflating: /content/lyrics/dolly-parton.txt  \n",
            "  inflating: /content/lyrics/dr-seuss.txt  \n",
            "  inflating: /content/lyrics/drake.txt  \n",
            "  inflating: /content/lyrics/eminem.txt  \n",
            "  inflating: /content/lyrics/janisjoplin.txt  \n",
            "  inflating: /content/lyrics/jimi-hendrix.txt  \n",
            "  inflating: /content/lyrics/johnny-cash.txt  \n",
            "  inflating: /content/lyrics/joni-mitchell.txt  \n",
            "  inflating: /content/lyrics/kanye-west.txt  \n",
            "  inflating: /content/lyrics/kanye.txt  \n",
            "  inflating: /content/lyrics/Kanye_West.txt  \n",
            "  inflating: /content/lyrics/lady-gaga.txt  \n",
            "  inflating: /content/lyrics/leonard-cohen.txt  \n",
            "  inflating: /content/lyrics/lil-wayne.txt  \n",
            "  inflating: /content/lyrics/Lil_Wayne.txt  \n",
            "  inflating: /content/lyrics/lin-manuel-miranda.txt  \n",
            "  inflating: /content/lyrics/lorde.txt  \n",
            "  inflating: /content/lyrics/ludacris.txt  \n",
            "  inflating: /content/lyrics/michael-jackson.txt  \n",
            "  inflating: /content/lyrics/missy-elliott.txt  \n",
            "  inflating: /content/lyrics/nickelback.txt  \n",
            "  inflating: /content/lyrics/nicki-minaj.txt  \n",
            "  inflating: /content/lyrics/nirvana.txt  \n",
            "  inflating: /content/lyrics/notorious-big.txt  \n",
            "  inflating: /content/lyrics/notorious_big.txt  \n",
            "  inflating: /content/lyrics/nursery_rhymes.txt  \n",
            "  inflating: /content/lyrics/patti-smith.txt  \n",
            "  inflating: /content/lyrics/paul-simon.txt  \n",
            "  inflating: /content/lyrics/prince.txt  \n",
            "  inflating: /content/lyrics/r-kelly.txt  \n",
            "  inflating: /content/lyrics/radiohead.txt  \n",
            "  inflating: /content/lyrics/rihanna.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt_file_path = '/content/lyrics/*'\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "raw_corpus = []\n",
        "\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "id": "4jEP-JdW9_TT",
        "outputId": "b6b15a65-ae1e-4bf3-8a28-dd0e166bdd76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['They say get ready for the revolution', \"I think it's time we find some sorta solution\", \"Somebody's caught up in the endless pollution\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-4. 데이터 전처리하기"
      ],
      "metadata": {
        "id": "dZ29Iq-_QGFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- corpus 전처리"
      ],
      "metadata": {
        "id": "oaUy3czJbxIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15 개 까지만....\n",
        "\n",
        "max = 15\n",
        "for i in range(len(corpus)):\n",
        "  count = corpus[i][:corpus[i].find('<end>')].count(' ') + 1\n",
        "  if count > max: numRemoved += 1"
      ],
      "metadata": {
        "id": "jKd0ciNGVDw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "QaS2QSQOhyyQ",
        "outputId": "a72a1756-feed-4be4-ca92-a91540076e46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156013"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0:10]"
      ],
      "metadata": {
        "id": "jpQ4CN1Uh1MZ",
        "outputId": "a7cc6674-f05c-4a5b-ae5a-ca9ede72cf5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> they say get ready for the revolution <end>',\n",
              " '<start> i think it s time we find some sorta solution <end>',\n",
              " '<start> somebody s caught up in the endless pollution <end>',\n",
              " '<start> why won t somebody feel this <end>',\n",
              " '<start> this is my wish that we all feel connected <end>',\n",
              " '<start> this is my wish that nobodies neglected be like a rocket baby <end>',\n",
              " '<start> be like a rocket take off <end>',\n",
              " '<start> just fly , away ay , ay <end>',\n",
              " '<start> to find your space take off <end>',\n",
              " '<start> just fly , away ay , ay <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 15\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue            # 문장길이 0\n",
        "    if sentence[-1] == \":\": continue           # 문장끝 :\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    # 15개 까지만...\n",
        "    if (preprocessed_sentence[:preprocessed_sentence.find('<end>')].count(' ') + 1) > MAX_LEN: \n",
        "        print(sentence)\n",
        "        continue\n",
        "    corpus.append(preprocessed_sentence)"
      ],
      "metadata": {
        "id": "t0_Fj3gWGvB_"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0:10]"
      ],
      "metadata": {
        "id": "zYQMX3pXJPJi",
        "outputId": "78c45bf0-a95d-4c5c-c314-1b479bf1bcf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> they say get ready for the revolution <end>',\n",
              " '<start> i think it s time we find some sorta solution <end>',\n",
              " '<start> somebody s caught up in the endless pollution <end>',\n",
              " '<start> why won t somebody feel this <end>',\n",
              " '<start> this is my wish that we all feel connected <end>',\n",
              " '<start> this is my wish that nobodies neglected be like a rocket baby <end>',\n",
              " '<start> be like a rocket take off <end>',\n",
              " '<start> just fly , away ay , ay <end>',\n",
              " '<start> to find your space take off <end>',\n",
              " '<start> just fly , away ay , ay <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "ix-6zSy6htd8",
        "outputId": "5b6f06f3-c232-4d0a-ace6-5c9787c2510d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156013"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numRemoved = 0\n",
        "max = 15\n",
        "for i in range(len(corpus)):\n",
        "  count = corpus[i][:corpus[i].find('<end>')].count(' ') + 1\n",
        "  if count > max: max = count\n",
        "\n",
        "max"
      ],
      "metadata": {
        "id": "zyRp4Lt0UR4I",
        "outputId": "fc310f99-0df6-45a7-e661-c0cf166ae5c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numRemoved = 0\n",
        "max = 15\n",
        "for i in range(len(corpus)):\n",
        "  count = corpus[i][:corpus[i].find('<end>')].count(' ') + 1\n",
        "  if count > max: numRemoved += 1\n",
        "\n",
        "numRemoved"
      ],
      "metadata": {
        "id": "ulIFGVtUUesY",
        "outputId": "d6c15f2b-869c-47fb-afe7-8330f243d617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텐서만들기"
      ],
      "metadata": {
        "id": "cREkHISvG2oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "id": "K1zKe6tZS6pa",
        "outputId": "a335a2c1-a454-446b-eba8-21200ec5f0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2   38   71 ...    0    0    0]\n",
            " [   2    4  130 ...    0    0    0]\n",
            " [   2  246   17 ...    0    0    0]\n",
            " ...\n",
            " [   2   20  149 ...    0    0    0]\n",
            " [   2    4   35 ...    3    0    0]\n",
            " [   2 1061   10 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fd23504e790>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensor분석하기"
      ],
      "metadata": {
        "id": "tnpYH4SkTLOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다."
      ],
      "metadata": {
        "id": "MFQ1eJGiWOjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " tokenizer.num_words"
      ],
      "metadata": {
        "id": "HGu8gvJzVJoK",
        "outputId": "dbf1ed0b-b4e3-4e35-e156-c652c5c12653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7000"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tensor[0])"
      ],
      "metadata": {
        "id": "BtSTk_qbId_z",
        "outputId": "52bc1d8e-37cb-4da1-e5eb-df21ef86ddd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor[0,:20]"
      ],
      "metadata": {
        "id": "cMLSJ7YmIFzF",
        "outputId": "9b132d7f-7f5e-4073-a95f-0462e094f6d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,   38,   71,   43,  294,   28,    6, 3273,    3,    0,    0,\n",
              "          0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor[:3, :20])"
      ],
      "metadata": {
        "id": "DgYgf8uQTNx-",
        "outputId": "9846de3c-f8d7-45e0-f3bd-bad5e497a9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2   38   71   43  294   28    6 3273    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2    4  130   11   17   76   21  207   94 3521 6826    3    0    0\n",
            "     0]\n",
            " [   2  246   17  622   29   14    6 3069    1    3    0    0    0    0\n",
            "     0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LSTM에서 many to many의 답을 얻을 것이기 때문에 train은 첫 문장부터 끝에 하나뺀 문장들로 구성을 하고 target은 첫 단어 뺀 문장들로 구성을 합니다,"
      ],
      "metadata": {
        "id": "hx7d3s8ygcJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]\n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]\n",
        "\n"
      ],
      "metadata": {
        "id": "_5tbmV8jIFR9"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "id": "BeHakRruTc-F",
        "outputId": "48f20ff6-f4f7-4a6e-e58b-c16af0584524",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2   38   71   43  294   28    6 3273    3    0    0    0    0    0]\n",
            "[  38   71   43  294   28    6 3273    3    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-5. validataion용 data 생성하기"
      ],
      "metadata": {
        "id": "v8Re96eRaZn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사이킷 런 패키지를 이용해서 위에서 train과 target을 train과 validation 셋으로 분리를 해줍니다. 8:2로 분리를 하겠습니다."
      ],
      "metadata": {
        "id": "Y-mEK6a9gmZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 8:2로 나누기\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=22)"
      ],
      "metadata": {
        "id": "lRS1hFtCS44q"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있습니다."
      ],
      "metadata": {
        "id": "eRrBg9_bXq9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 모델 설계"
      ],
      "metadata": {
        "id": "kXWJ5i6Ir7KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-1. corpus 텐서를 tf.data.Dataset객체로 변환"
      ],
      "metadata": {
        "id": "6iP_4pj7bXvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fdsafdsa"
      ],
      "metadata": {
        "id": "M2aFutfJr4OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "VOCAB_SIZE = tokenizer.num_words + 1         # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "\n",
        "model = TextGenerator(VOCAB_SIZE, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "Jhq66CGLbigo"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "# optimizer와 loss등은 차차 배웁니다\n",
        "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "model.compile(loss=loss, optimizer=optimizer)"
      ],
      "metadata": {
        "id": "vmuFWLsKbUOf"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 문제점: 인력덴서가 무엇인지 미정상태\n",
        "- model에 데이터를 아주 조금 태워 보는 것도 방법입니다. model의 input shape가 결정되면서 model.build()가 자동으로 호출됩니다."
      ],
      "metadata": {
        "id": "Z7OXj_k2YIOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "model(src_sample)"
      ],
      "metadata": {
        "id": "Jm4VqZbxLAZs",
        "outputId": "54fc7c26-2079-4bc6-8e90-11b790a4b201",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 7001), dtype=float32, numpy=\n",
              "array([[[-2.67563519e-05, -5.68164687e-05,  4.22008132e-04, ...,\n",
              "         -5.30208490e-06,  4.35386755e-04,  2.76740146e-04],\n",
              "        [-2.99895299e-04,  2.61646230e-04,  7.11476721e-04, ...,\n",
              "         -2.44583527e-04,  9.49296402e-04,  3.94055416e-04],\n",
              "        [-3.47216584e-04,  6.71739108e-04,  5.72921243e-04, ...,\n",
              "         -2.69298500e-04,  1.36719621e-03,  2.19789625e-04],\n",
              "        ...,\n",
              "        [ 4.92481457e-04, -3.41615261e-04, -2.15352885e-03, ...,\n",
              "         -1.10786175e-03, -5.96851460e-05, -1.06951094e-03],\n",
              "        [ 7.42310134e-04, -1.59527583e-04, -2.33880244e-03, ...,\n",
              "         -1.50653801e-03, -5.10848477e-04, -1.21292495e-03],\n",
              "        [ 7.97900022e-04,  2.56008789e-04, -2.29813159e-03, ...,\n",
              "         -1.41283660e-03, -8.55471473e-04, -1.31641084e-03]],\n",
              "\n",
              "       [[-2.67563519e-05, -5.68164687e-05,  4.22008132e-04, ...,\n",
              "         -5.30208490e-06,  4.35386755e-04,  2.76740146e-04],\n",
              "        [-7.91184721e-06,  3.05956608e-04,  7.60992290e-04, ...,\n",
              "         -1.80429051e-04,  1.14429323e-03,  1.71578838e-04],\n",
              "        [ 2.01566159e-04,  3.14645062e-04,  5.91940829e-04, ...,\n",
              "         -4.15737974e-04,  1.37371523e-03,  1.58278810e-04],\n",
              "        ...,\n",
              "        [ 1.00391524e-04, -7.77023146e-04, -5.20449132e-04, ...,\n",
              "          2.06434081e-04,  1.26362647e-04,  6.46336004e-04],\n",
              "        [ 1.73598688e-04, -7.92307896e-04, -6.81767589e-04, ...,\n",
              "          3.50340386e-04,  1.24340644e-04,  6.66142732e-04],\n",
              "        [ 2.67062074e-04, -8.47891788e-04, -7.79927359e-04, ...,\n",
              "          4.82540810e-04,  2.60662287e-04,  7.35054142e-04]],\n",
              "\n",
              "       [[-2.67563519e-05, -5.68164687e-05,  4.22008132e-04, ...,\n",
              "         -5.30208490e-06,  4.35386755e-04,  2.76740146e-04],\n",
              "        [ 6.41714723e-05, -2.83347908e-04,  8.25238531e-04, ...,\n",
              "          2.14291431e-04,  6.25249522e-04,  4.98288951e-04],\n",
              "        [ 1.16353593e-04, -3.18970764e-04,  8.15907842e-04, ...,\n",
              "          7.02559599e-04,  8.40302906e-04,  6.27808680e-04],\n",
              "        ...,\n",
              "        [ 2.62314454e-04, -7.40300282e-04,  7.78817630e-05, ...,\n",
              "          1.13639049e-03,  1.26321462e-03,  5.38252527e-04],\n",
              "        [ 2.91246688e-04, -7.84725707e-04,  3.98342090e-05, ...,\n",
              "          1.10425125e-03,  1.47226243e-03,  6.71463960e-04],\n",
              "        [ 3.25449917e-04, -8.43536342e-04,  2.79409287e-05, ...,\n",
              "          1.05406600e-03,  1.65341422e-03,  7.91631872e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-2.67563519e-05, -5.68164687e-05,  4.22008132e-04, ...,\n",
              "         -5.30208490e-06,  4.35386755e-04,  2.76740146e-04],\n",
              "        [-3.05376016e-04, -3.68803914e-04,  7.62231182e-04, ...,\n",
              "          3.28165479e-05,  6.77832286e-04,  3.49261216e-04],\n",
              "        [-6.29494432e-04, -8.32971011e-04,  1.10701099e-03, ...,\n",
              "          3.27520102e-05,  8.34732200e-04,  3.62031657e-04],\n",
              "        ...,\n",
              "        [ 6.75420626e-04, -9.95465438e-04,  4.78365197e-04, ...,\n",
              "          7.79283699e-04,  1.97023642e-03,  1.12385349e-03],\n",
              "        [ 7.11117114e-04, -1.09409203e-03,  4.23978956e-04, ...,\n",
              "          7.96446227e-04,  2.07080320e-03,  1.17214560e-03],\n",
              "        [ 7.32883636e-04, -1.19414576e-03,  3.86614469e-04, ...,\n",
              "          7.98856141e-04,  2.15147622e-03,  1.20792363e-03]],\n",
              "\n",
              "       [[-2.67563519e-05, -5.68164687e-05,  4.22008132e-04, ...,\n",
              "         -5.30208490e-06,  4.35386755e-04,  2.76740146e-04],\n",
              "        [-2.08947677e-04,  7.45503348e-05,  5.30970923e-04, ...,\n",
              "         -9.20492166e-05,  4.67689941e-04,  2.16159970e-04],\n",
              "        [-1.63273333e-04,  4.78254515e-06,  5.87415067e-04, ...,\n",
              "         -2.16778542e-04,  3.56994191e-04, -2.22350645e-05],\n",
              "        ...,\n",
              "        [ 4.63266857e-04, -7.01209239e-04,  9.74379655e-05, ...,\n",
              "          6.02766057e-04,  1.24897901e-03,  7.72136205e-04],\n",
              "        [ 5.04738884e-04, -8.32914142e-04,  1.22959551e-04, ...,\n",
              "          6.39871811e-04,  1.42747909e-03,  8.69615935e-04],\n",
              "        [ 5.40754409e-04, -9.60910460e-04,  1.57806207e-04, ...,\n",
              "          6.56497781e-04,  1.58793037e-03,  9.50823887e-04]],\n",
              "\n",
              "       [[-2.67563519e-05, -5.68164687e-05,  4.22008132e-04, ...,\n",
              "         -5.30208490e-06,  4.35386755e-04,  2.76740146e-04],\n",
              "        [-1.98371636e-04,  3.97563854e-05,  4.30770160e-04, ...,\n",
              "         -3.52638090e-05,  5.79088344e-04,  3.07425798e-04],\n",
              "        [-2.53061124e-04, -3.19787359e-05,  4.17393225e-04, ...,\n",
              "         -8.29189448e-05,  5.05287200e-04,  9.57586453e-05],\n",
              "        ...,\n",
              "        [-7.09056039e-04,  3.05214606e-04,  9.33837204e-04, ...,\n",
              "          6.56652264e-04,  9.98383912e-05, -4.92767314e-04],\n",
              "        [-6.23883854e-04,  3.44649656e-04,  7.15277216e-04, ...,\n",
              "          8.52795609e-04, -2.08301470e-04, -8.03578878e-04],\n",
              "        [-4.55152534e-04,  2.36931490e-04,  5.31376863e-04, ...,\n",
              "          9.36404627e-04, -2.73459882e-04, -8.58782791e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch"
      ],
      "metadata": {
        "id": "ky3AS6Majgck",
        "outputId": "3cf20641-0fa7-44cb-b6c7-403b45e3d070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "609"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델의 최종 출력 텐서 shape를 유심히 보면 shape=(256, 14, 7001)임을 알 수 있습니다. \n",
        "- 7001은 Dense 레이어의 출력 차원수입니다. \n",
        "- 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야 하기 때문입니다.\n",
        "- 256은 이전 스텝에서 지정한 배치 사이즈입니다. dataset.take(1)를 통해서 1개의 배치, 즉 256개의 문장 데이터를 가져온 것입니다."
      ],
      "metadata": {
        "id": "m0qKrAvkY5t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 그렇다면 14는 무엇을 의미할까요? 비밀은 바로 tf.keras.layers.LSTM(hidden_size, return_sequences=True)로 호출한 LSTM 레이어에서 return_sequences=True이라고 지정한 부분에 있습니다. \n",
        "- 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미입니다. 만약 return_sequences=False였다면 LSTM 레이어는 1개의 벡터만 출력했을 것입니다."
      ],
      "metadata": {
        "id": "oOKugYWcY6dI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 그런데 문제는, 우리의 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모른다는 점입니다. 모델을 만들면서 알려준 적도 없습니다. 그럼 14는 언제 알게된 것일까요? \n",
        "- 네, 그렇습니다. 데이터를 입력받으면서 비로소 알게 된 것입니다. 우리 데이터셋의 max_len이 14으로 맞춰져 있었던 것입니다."
      ],
      "metadata": {
        "id": "98EdJhDzZHZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "tf.keras.utils.plot_model(\n",
        "    model, \n",
        "    show_shapes=True, \n",
        "    show_layer_names=True,\n",
        "    dpi = 70)"
      ],
      "metadata": {
        "id": "NkCi-giYLF4j",
        "outputId": "7c348f7b-5577-472a-f138-f0d3c06247d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  1792256   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  7176025   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,607,961\n",
            "Trainable params: 22,607,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 우리의 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없는 것입니다."
      ],
      "metadata": {
        "id": "TH_b3b4cZQox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 모델 학습"
      ],
      "metadata": {
        "id": "gAbT7tesYp3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "\n",
        "history = model.fit(enc_train, dec_train, \n",
        "          epochs=epochs,                         # 10\n",
        "          batch_size=BATCH_SIZE,                 # 256\n",
        "          validation_data=(enc_val, dec_val),\n",
        "          verbose=1)"
      ],
      "metadata": {
        "id": "8cNBXHBzZafG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이번처럼 작문 모델을 평가하는 가장 확실한 방법은 작문을 시켜보고 사람이 평가하는 겁니다.\n",
        "- 아래 generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 합니다."
      ],
      "metadata": {
        "id": "OQ1h3z6UZr6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 모델 평가"
      ],
      "metadata": {
        "id": "ZXzx1CsCaPCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OFvq5NUkKyAy",
        "outputId": "234185f1-458e-43dd-aa1f-e49a3271f988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love rubies bloodstains ceased ceased busy busy cab cab pearls pearls had had sp sp steady steady steady '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UzmD2htluydd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "    \n",
        "    fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
        "    \n",
        "    for i in range(len(ax)):\n",
        "        ax[i].set_xlabel('Epochs')\n",
        "        ax[i].set_ylabel('Value')\n",
        "        \n",
        "        for n in range(len(list_of_metrics)):\n",
        "            if i == 0:\n",
        "                y = hist[list_of_metrics[n]]\n",
        "                if n == 0:\n",
        "                    ax[i].plot(epochs, y, label=\"train\")\n",
        "                else:\n",
        "                    ax[i].plot(epochs, y, label=\"val\")\n",
        "                ax[i].set_title('Loss')\n",
        "                ax[i].legend(loc='upper right')\n",
        "                if n == 1:\n",
        "                    break\n",
        "            else:\n",
        "                if n >= 2:\n",
        "                    y = hist[list_of_metrics[n]]\n",
        "                    if n == 2:\n",
        "                        ax[i].plot(epochs, y, label=\"train\")\n",
        "                    else:\n",
        "                        ax[i].plot(epochs, y, label=\"val\")\n",
        "                    ax[i].set_title('Accuracy')\n",
        "                    ax[i].legend(loc='lower right')\n",
        "                    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "63qWXOa5uzA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curve(history.epoch, history.history, ['loss', 'val_loss'])"
      ],
      "metadata": {
        "id": "1E5zCBhru6iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그래프를 보니 train은 학습하면 할 수록 과적합이 되서 그런지 점점 loss가 줄어드는 모습을 보입니다. 그리고 validation에서의 loss는 train의 학습으로는 한계가 있는지 점점 loss가 줄어드는 폭이 좁아지는 것을 볼 수 있었습니다."
      ],
      "metadata": {
        "id": "lSmH5g9IvQu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "더 좋은 결과를 얻기 위해 어떤 과정이 남아 있을까?\n",
        "\n",
        "    lstm의 하이퍼 파라미터를 수정 또는 층을 늘린다.\n",
        "    epochs에 earlystopping을 추가하여 가장 강력할 때 멈춘다.\n",
        "    각 lstm 층마다 과적합 방지 기법을 사용한다. ( drop out, batchnormalization)\n",
        "    cross validation을 사용한다.\n",
        "    optimizer를 Adam말고 시퀀스 데이터에 더 적합하다고 알려진 rmsprop를 사용한다.\n"
      ],
      "metadata": {
        "id": "kp_7pSK4vYB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 회고"
      ],
      "metadata": {
        "id": "D83tA9zBuilT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- lstm을 진행하면서 lstm을 3층으로도 쌓고 dence를 2단으로 쌓아 더 강력하게하고 정규화 기법인 drop out이나 batchnormalization 등을 사용하여 단순하게 lstm을 돌렸을 때와 비교를 해보고 싶었지만 코드를 만들어본 결과 하나하나의 epochs가 시간이 너무 오래걸리기도 하고 많은 파라미터를 처리해야 하기 때문에 메모리가 부족할 것 같아서 진행을 못한 점이 아쉽습니다. \n",
        "- 어쨋든 간단한 lstm으로 간단한 데이터를 인풋으로 했을 때 적절한 답변을 얻는 모습을 볼 수 있었습니다. \n",
        "- 학습시간만 조금 짧았더라면 많은 시도를 했었을 것 같은데 아쉽네요!"
      ],
      "metadata": {
        "id": "KaFmnrQPulgd"
      }
    }
  ]
}