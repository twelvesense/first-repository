{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExplNode06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/wfZkgtgI4L5ANhuveJYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twelvesense/first-repository/blob/master/ExplNode06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트: 멋진 작사가 만들기"
      ],
      "metadata": {
        "id": "H7Y58OA38ag3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 개요"
      ],
      "metadata": {
        "id": "ZNi-HieyNthJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1. 목표\n",
        "- 입력 한 단어로, 문장을 만드는 LSTM이용한 NN을 구현한다.\n",
        "- loss 2.2 이하"
      ],
      "metadata": {
        "id": "mYzHL5k9PUiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. 방법\n",
        "- NN구성: Embedding 레이어 1개, LSTM 레이어 2개, Dense 레이어 1개 순\n",
        "- tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 tensor로 변환\n",
        "- tf.data.Dataset.from_tensor_slices()를 이용해 tensor를 tf.data.Dataset객체로 변환\n",
        "- 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수 있기에, corpus기준으로 token수를 15로 제한\n",
        "- 하이퍼파라미터를 tuning한다."
      ],
      "metadata": {
        "id": "NqYeus28PV8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-3. 사전 준비"
      ],
      "metadata": {
        "id": "EzSDEIHIpgfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ■ 기본 라이브러리 load하기"
      ],
      "metadata": {
        "id": "GDCfgdRvppX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "# import pandas as pd\n",
        "import numpy as np\n",
        "import os, re\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "6rBBxFLBpkyQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ■ 기본 code load하기"
      ],
      "metadata": {
        "id": "NLHYFHmap4Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2. 특수문자 양쪽에 공백을 넣고\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #  4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "    sentence = sentence.strip() # 5. 다시 양쪽 공백을 지웁니다\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "    return sentence\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=7000,               # 7000단어를 기억할 수 있는 tokenizer\n",
        "        filters=' ',                  # 앞에서 preprocess하므로, 여기서는 불필요\n",
        "        oov_token=\"<unk>\"             # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿈\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)  \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "\n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence를 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated  "
      ],
      "metadata": {
        "id": "p76-YBxMp9sl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 데이터 준비"
      ],
      "metadata": {
        "id": "QqzFKCv3Aqjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. raw corpus 파일 download하기"
      ],
      "metadata": {
        "id": "o1udTHDCAx9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/lyrics\n",
        "!mkdir -p /content/lyrics\n",
        "!wget -O /content/lyrics.zip https://raw.githubusercontent.com/twelvesense/first-repository/master/data/lyrics/lyrics.zip\n",
        "!unzip /content/lyrics.zip -d /content/lyrics\n",
        "!rm /content/lyrics.zip"
      ],
      "metadata": {
        "id": "gpoAcRyWA-9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1062efc3-a403-45e6-b2c3-61b1ae220adb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-19 14:26:55--  https://raw.githubusercontent.com/twelvesense/first-repository/master/data/lyrics/lyrics.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2004593 (1.9M) [application/zip]\n",
            "Saving to: ‘/content/lyrics.zip’\n",
            "\n",
            "/content/lyrics.zip 100%[===================>]   1.91M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-05-19 14:26:55 (32.8 MB/s) - ‘/content/lyrics.zip’ saved [2004593/2004593]\n",
            "\n",
            "Archive:  /content/lyrics.zip\n",
            "  inflating: /content/lyrics/adele.txt  \n",
            "  inflating: /content/lyrics/al-green.txt  \n",
            "  inflating: /content/lyrics/alicia-keys.txt  \n",
            "  inflating: /content/lyrics/amy-winehouse.txt  \n",
            "  inflating: /content/lyrics/beatles.txt  \n",
            "  inflating: /content/lyrics/bieber.txt  \n",
            "  inflating: /content/lyrics/bjork.txt  \n",
            "  inflating: /content/lyrics/blink-182.txt  \n",
            "  inflating: /content/lyrics/bob-dylan.txt  \n",
            "  inflating: /content/lyrics/bob-marley.txt  \n",
            "  inflating: /content/lyrics/britney-spears.txt  \n",
            "  inflating: /content/lyrics/bruce-springsteen.txt  \n",
            "  inflating: /content/lyrics/bruno-mars.txt  \n",
            "  inflating: /content/lyrics/cake.txt  \n",
            "  inflating: /content/lyrics/dickinson.txt  \n",
            "  inflating: /content/lyrics/disney.txt  \n",
            "  inflating: /content/lyrics/dj-khaled.txt  \n",
            "  inflating: /content/lyrics/dolly-parton.txt  \n",
            "  inflating: /content/lyrics/dr-seuss.txt  \n",
            "  inflating: /content/lyrics/drake.txt  \n",
            "  inflating: /content/lyrics/eminem.txt  \n",
            "  inflating: /content/lyrics/janisjoplin.txt  \n",
            "  inflating: /content/lyrics/jimi-hendrix.txt  \n",
            "  inflating: /content/lyrics/johnny-cash.txt  \n",
            "  inflating: /content/lyrics/joni-mitchell.txt  \n",
            "  inflating: /content/lyrics/kanye-west.txt  \n",
            "  inflating: /content/lyrics/kanye.txt  \n",
            "  inflating: /content/lyrics/Kanye_West.txt  \n",
            "  inflating: /content/lyrics/lady-gaga.txt  \n",
            "  inflating: /content/lyrics/leonard-cohen.txt  \n",
            "  inflating: /content/lyrics/lil-wayne.txt  \n",
            "  inflating: /content/lyrics/Lil_Wayne.txt  \n",
            "  inflating: /content/lyrics/lin-manuel-miranda.txt  \n",
            "  inflating: /content/lyrics/lorde.txt  \n",
            "  inflating: /content/lyrics/ludacris.txt  \n",
            "  inflating: /content/lyrics/michael-jackson.txt  \n",
            "  inflating: /content/lyrics/missy-elliott.txt  \n",
            "  inflating: /content/lyrics/nickelback.txt  \n",
            "  inflating: /content/lyrics/nicki-minaj.txt  \n",
            "  inflating: /content/lyrics/nirvana.txt  \n",
            "  inflating: /content/lyrics/notorious-big.txt  \n",
            "  inflating: /content/lyrics/notorious_big.txt  \n",
            "  inflating: /content/lyrics/nursery_rhymes.txt  \n",
            "  inflating: /content/lyrics/patti-smith.txt  \n",
            "  inflating: /content/lyrics/paul-simon.txt  \n",
            "  inflating: /content/lyrics/prince.txt  \n",
            "  inflating: /content/lyrics/r-kelly.txt  \n",
            "  inflating: /content/lyrics/radiohead.txt  \n",
            "  inflating: /content/lyrics/rihanna.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt_file_path = '/content/lyrics/*'\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "raw_corpus = []\n",
        "\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)"
      ],
      "metadata": {
        "id": "forhEnlCt2kT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. corpus 파일 분석하기"
      ],
      "metadata": {
        "id": "y5GOuRjMqPwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\")\n",
        "raw_corpus[:10]"
      ],
      "metadata": {
        "id": "4jEP-JdW9_TT",
        "outputId": "a0209892-f76d-4707-f420-d7ddeb81283b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['They say get ready for the revolution',\n",
              " \"I think it's time we find some sorta solution\",\n",
              " \"Somebody's caught up in the endless pollution\",\n",
              " 'They need to wake up, stop living illusions I know you need to hear this',\n",
              " \"Why won't somebody feel this\",\n",
              " 'This is my wish that we all feel connected',\n",
              " 'This is my wish that nobodies neglected Be like a rocket baby',\n",
              " 'Be like a rocket Take off',\n",
              " 'Just fly, away (ay, ay)',\n",
              " 'To find your space Take off']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-4. 데이터 전처리하기"
      ],
      "metadata": {
        "id": "dZ29Iq-_QGFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ■ corpus 생성 시, sentence별 token수를 최대 15개로 제한하기"
      ],
      "metadata": {
        "id": "oaUy3czJbxIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 15                                  # sentence별 최대 token수\n",
        "num_removed = 0                               # token수가 15초과하는 sentence의 수\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue            # 문장길이 0\n",
        "    if sentence[-1] == \":\": continue           # 문장끝 :\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    # 15개 까지만...\n",
        "    if (preprocessed_sentence[:preprocessed_sentence.find('<end>')].count(' ') + 1) > MAX_LEN: \n",
        "        num_removed += 1\n",
        "        continue\n",
        "    corpus.append(preprocessed_sentence)"
      ],
      "metadata": {
        "id": "t0_Fj3gWGvB_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_removed                                     # token수가 15초과하여 corpus에 제외된 sentence의 수"
      ],
      "metadata": {
        "id": "PnL4rUXEvegQ",
        "outputId": "b2e9a9e8-9618-410a-dc9b-ce1c5bb3ca85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19736"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"데이터 크기:\", len(corpus))\n",
        "print(\"Examples:\")\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYQMX3pXJPJi",
        "outputId": "b44a3cc0-27e6-4444-e33e-168a7c2d1fd5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 156013\n",
            "Examples:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> they say get ready for the revolution <end>',\n",
              " '<start> i think it s time we find some sorta solution <end>',\n",
              " '<start> somebody s caught up in the endless pollution <end>',\n",
              " '<start> why won t somebody feel this <end>',\n",
              " '<start> this is my wish that we all feel connected <end>',\n",
              " '<start> this is my wish that nobodies neglected be like a rocket baby <end>',\n",
              " '<start> be like a rocket take off <end>',\n",
              " '<start> just fly , away ay , ay <end>',\n",
              " '<start> to find your space take off <end>',\n",
              " '<start> just fly , away ay , ay <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-5. tensor 생성하기"
      ],
      "metadata": {
        "id": "cREkHISvG2oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "id": "K1zKe6tZS6pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1ea89f-3efd-4da0-e26b-ef79c79ea1f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2   38   71 ...    0    0    0]\n",
            " [   2    4  130 ...    0    0    0]\n",
            " [   2  246   17 ...    0    0    0]\n",
            " ...\n",
            " [   2   20  149 ...    0    0    0]\n",
            " [   2    4   35 ...    3    0    0]\n",
            " [   2 1061   10 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7efe96c62350>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-6. tensor 분석하기"
      ],
      "metadata": {
        "id": "tnpYH4SkTLOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtSTk_qbId_z",
        "outputId": "e35604f6-4bc3-4e70-f915-8aaf75e452f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156013"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor[0:10,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMLSJ7YmIFzF",
        "outputId": "05bb9a4b-0967-4bf8-d272-961f5eac6de0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2,   38,   71,   43,  294,   28,    6, 3273,    3,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   2,    4,  130,   11,   17,   76,   21,  207,   94, 3521, 6826,\n",
              "           3,    0,    0,    0],\n",
              "       [   2,  246,   17,  622,   29,   14,    6, 3069,    1,    3,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   2,  119,  178,   16,  246,  106,   41,    3,    0,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   2,   41,   26,   13,  275,   15,   21,   25,  106, 5061,    3,\n",
              "           0,    0,    0,    0],\n",
              "       [   2,   41,   26,   13,  275,   15,    1,    1,   27,   23,    9,\n",
              "        2967,   52,    3,    0],\n",
              "       [   2,   27,   23,    9, 2967,   83,  117,    3,    0,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   2,   32,  254,    5,  138,  610,    5,  610,    3,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   2,   10,  207,   19,  689,   83,  117,    3,    0,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   2,   32,  254,    5,  138,  610,    5,  610,    3,    0,    0,\n",
              "           0,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-7. source 문장, target 문장 만들기"
      ],
      "metadata": {
        "id": "hx7d3s8ygcJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 token을 잘라내서 source 문장을 생성\n",
        "# 마지막 token은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]\n",
        "# tensor에서 <start>를 잘라내서 target 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]"
      ],
      "metadata": {
        "id": "_5tbmV8jIFR9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "id": "BeHakRruTc-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a835b17-9554-4034-8719-45687e6184f8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2   38   71   43  294   28    6 3273    3    0    0    0    0    0]\n",
            "[  38   71   43  294   28    6 3273    3    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-8. validataion test용으로 dataset 나누기"
      ],
      "metadata": {
        "id": "v8Re96eRaZn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# validataion test용으로 8:2로 나누기\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=22)"
      ],
      "metadata": {
        "id": "lRS1hFtCS44q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 모델 설계"
      ],
      "metadata": {
        "id": "kXWJ5i6Ir7KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-1. Model Subclassing 방식으로 구현"
      ],
      "metadata": {
        "id": "6iP_4pj7bXvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ■ 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성하기"
      ],
      "metadata": {
        "id": "M2aFutfJr4OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 14\n",
        "hidden_size = 2048\n",
        "VOCAB_SIZE = tokenizer.num_words + 1         # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "\n",
        "model = TextGenerator(VOCAB_SIZE, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "Jhq66CGLbigo",
        "outputId": "efe2d135-1005-4706-eaa6-f61a1189b4ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-002684e6b441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mVOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m         \u001b[0;31m# tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-2. 기타 하이퍼파라미터 설정하기"
      ],
      "metadata": {
        "id": "--A0aTnV1kpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "model.compile(loss=loss, optimizer=optimizer)"
      ],
      "metadata": {
        "id": "vmuFWLsKbUOf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ■ model에 일부 데이터를 태워 input shape 확인하기"
      ],
      "metadata": {
        "id": "Z7OXj_k2YIOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "model(src_sample)"
      ],
      "metadata": {
        "id": "Jm4VqZbxLAZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4000f3f3-d704-4911-dfe7-8faac5ec437c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 7001), dtype=float32, numpy=\n",
              "array([[[ 8.14504601e-05, -3.02440487e-04, -1.89831655e-04, ...,\n",
              "         -1.96397799e-04, -1.13841365e-04,  6.77626158e-05],\n",
              "        [-7.89928308e-05, -1.61608739e-04, -2.60059576e-04, ...,\n",
              "         -4.43589728e-04, -4.06103441e-04,  5.90616364e-05],\n",
              "        [-3.04575806e-04, -2.27130280e-04, -6.47073903e-05, ...,\n",
              "         -5.79385669e-04, -4.47164814e-04,  2.75851751e-04],\n",
              "        ...,\n",
              "        [-1.43077597e-03, -1.79760507e-04,  1.09904876e-03, ...,\n",
              "         -4.12474037e-05, -1.20258890e-03,  2.99938582e-03],\n",
              "        [-1.37093908e-03, -2.63550261e-04,  1.19805813e-03, ...,\n",
              "          7.08283624e-05, -1.11084827e-03,  3.26795643e-03],\n",
              "        [-1.33784732e-03,  2.41704984e-05,  1.11488043e-03, ...,\n",
              "          6.72779861e-05, -1.32889755e-03,  3.32410773e-03]],\n",
              "\n",
              "       [[ 8.14504601e-05, -3.02440487e-04, -1.89831655e-04, ...,\n",
              "         -1.96397799e-04, -1.13841365e-04,  6.77626158e-05],\n",
              "        [ 3.79202073e-04, -3.23770335e-04, -8.86248890e-05, ...,\n",
              "         -2.79231259e-04, -5.32209233e-05, -2.58871063e-04],\n",
              "        [ 4.70217114e-04, -3.91860754e-04,  1.72609580e-04, ...,\n",
              "         -1.35755705e-04, -2.25835043e-04, -6.31885778e-04],\n",
              "        ...,\n",
              "        [-2.02625044e-04, -1.08479161e-03,  7.15908594e-04, ...,\n",
              "         -1.20954704e-04,  1.30574615e-03, -8.08734680e-04],\n",
              "        [-3.76048847e-04, -1.07458001e-03,  8.21033726e-04, ...,\n",
              "         -2.24012183e-04,  1.91344682e-03, -3.00575164e-04],\n",
              "        [-4.87283309e-04, -1.06387993e-03,  9.53524257e-04, ...,\n",
              "         -2.81708315e-04,  2.42730323e-03,  1.64285681e-04]],\n",
              "\n",
              "       [[ 8.14504601e-05, -3.02440487e-04, -1.89831655e-04, ...,\n",
              "         -1.96397799e-04, -1.13841365e-04,  6.77626158e-05],\n",
              "        [ 2.91513075e-04, -5.32975653e-04, -5.85299102e-04, ...,\n",
              "         -4.40330070e-04, -2.01510338e-04, -2.88435374e-04],\n",
              "        [ 2.14732601e-04, -7.80826842e-04, -6.96803560e-04, ...,\n",
              "         -7.37430528e-04, -5.35225437e-04, -3.33240780e-04],\n",
              "        ...,\n",
              "        [ 4.01934725e-04, -9.30859358e-04,  9.32497962e-04, ...,\n",
              "         -4.58479160e-04,  2.37444718e-03,  1.16819202e-03],\n",
              "        [ 2.28299585e-04, -1.02782540e-03,  1.18777889e-03, ...,\n",
              "         -3.71559290e-04,  2.78198393e-03,  1.47701916e-03],\n",
              "        [ 1.09288085e-04, -1.09344686e-03,  1.42517476e-03, ...,\n",
              "         -2.75321072e-04,  3.10944649e-03,  1.72180752e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 8.14504601e-05, -3.02440487e-04, -1.89831655e-04, ...,\n",
              "         -1.96397799e-04, -1.13841365e-04,  6.77626158e-05],\n",
              "        [ 1.17548560e-04, -6.07541821e-04, -4.84479562e-04, ...,\n",
              "         -3.77477816e-04,  5.14182466e-06,  1.03650789e-04],\n",
              "        [ 2.71794095e-04, -8.97047576e-04, -5.35547035e-04, ...,\n",
              "         -2.56689207e-04,  2.43063376e-04,  4.10876819e-05],\n",
              "        ...,\n",
              "        [ 2.34587293e-04, -6.97228184e-04,  2.74368795e-05, ...,\n",
              "          8.86608032e-05,  2.39471626e-03,  9.02830972e-04],\n",
              "        [ 1.63311139e-04, -7.44166085e-04,  2.85402843e-04, ...,\n",
              "         -1.22520723e-04,  2.88115744e-03,  1.20112218e-03],\n",
              "        [ 1.07324915e-04, -7.90236634e-04,  5.41228917e-04, ...,\n",
              "         -2.68121366e-04,  3.27063375e-03,  1.44710136e-03]],\n",
              "\n",
              "       [[ 8.14504601e-05, -3.02440487e-04, -1.89831655e-04, ...,\n",
              "         -1.96397799e-04, -1.13841365e-04,  6.77626158e-05],\n",
              "        [-2.95750622e-04,  4.27617706e-05, -2.45363219e-04, ...,\n",
              "         -1.90909297e-04, -1.39114898e-04, -2.30444508e-04],\n",
              "        [-7.25371821e-04,  5.35616535e-04, -2.04138603e-04, ...,\n",
              "         -2.04263881e-04, -2.37607514e-04, -9.83097591e-04],\n",
              "        ...,\n",
              "        [-1.63408846e-03,  2.96510232e-04,  1.50977005e-03, ...,\n",
              "         -7.76660745e-05,  2.57418142e-05, -1.04647852e-03],\n",
              "        [-1.64004834e-03,  4.72492684e-06,  1.64951652e-03, ...,\n",
              "         -2.74443475e-04,  5.46146824e-04, -4.13365749e-04],\n",
              "        [-1.59735652e-03, -2.71753728e-04,  1.74579443e-03, ...,\n",
              "         -4.04826133e-04,  1.10322377e-03,  1.80224233e-04]],\n",
              "\n",
              "       [[ 8.14504601e-05, -3.02440487e-04, -1.89831655e-04, ...,\n",
              "         -1.96397799e-04, -1.13841365e-04,  6.77626158e-05],\n",
              "        [ 9.15759738e-05, -1.61407515e-04, -4.63499397e-04, ...,\n",
              "         -3.82928527e-04,  2.91066008e-05, -3.29879986e-04],\n",
              "        [ 1.55615882e-04, -4.53066808e-04, -5.04653435e-04, ...,\n",
              "         -3.50727350e-06,  2.75097205e-04, -5.56848710e-04],\n",
              "        ...,\n",
              "        [-5.80418506e-04, -1.41529029e-03,  6.94669201e-04, ...,\n",
              "          4.51910309e-05,  3.52790300e-03,  1.64132146e-03],\n",
              "        [-5.47911914e-04, -1.46671361e-03,  9.72843322e-04, ...,\n",
              "          4.45123296e-05,  3.73530015e-03,  1.88538339e-03],\n",
              "        [-4.92826337e-04, -1.48513890e-03,  1.24119408e-03, ...,\n",
              "          6.66542910e-05,  3.87605629e-03,  2.06936267e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 출력 tensor shape은 (256, 14, 7001)임\n",
        "- 7001에는 token별 확률 표현\n",
        "- 14는 입력 sequence와 동일한 길이의 출력 sequence를 나타냄\n",
        "- 256은 이전 스텝에서 지정한 배치 사이즈"
      ],
      "metadata": {
        "id": "m0qKrAvkY5t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-3. 모델 summary"
      ],
      "metadata": {
        "id": "07nHXwJp5_tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "tf.keras.utils.plot_model(\n",
        "    model, \n",
        "    show_shapes=True, \n",
        "    show_layer_names=True,\n",
        "    dpi = 70)"
      ],
      "metadata": {
        "id": "NkCi-giYLF4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "df930f19-526d-4f8e-d5ce-0c805b9ea289"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  1792256   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  7176025   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,607,961\n",
            "Trainable params: 22,607,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 모델 학습"
      ],
      "metadata": {
        "id": "gAbT7tesYp3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "\n",
        "history = model.fit(enc_train, dec_train, \n",
        "          epochs=epochs,                         # 10\n",
        "          batch_size=BATCH_SIZE,                 # 256\n",
        "          validation_data=(enc_val, dec_val),\n",
        "          verbose=1)"
      ],
      "metadata": {
        "id": "8cNBXHBzZafG",
        "outputId": "670ec66d-ff14-4d3d-c450-4396fd507c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 83/488 [====>.........................] - ETA: 1:13:16 - loss: 4.1499"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-0534d0e69908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이번처럼 작문 모델을 평가하는 가장 확실한 방법은 작문을 시켜보고 사람이 평가하는 겁니다.\n",
        "- 아래 generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 합니다."
      ],
      "metadata": {
        "id": "OQ1h3z6UZr6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 모델 평가"
      ],
      "metadata": {
        "id": "ZXzx1CsCaPCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- generate_text 함수는 시작 단어(들)을 받은 모델이 작문을 하게 하는데, 사람이 평가하는 방법이 있다."
      ],
      "metadata": {
        "id": "O0tsfd7n8CUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OFvq5NUkKyAy",
        "outputId": "234185f1-458e-43dd-aa1f-e49a3271f988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love rubies bloodstains ceased ceased busy busy cab cab pearls pearls had had sp sp steady steady steady '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UzmD2htluydd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "    \n",
        "    fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
        "    \n",
        "    for i in range(len(ax)):\n",
        "        ax[i].set_xlabel('Epochs')\n",
        "        ax[i].set_ylabel('Value')\n",
        "        \n",
        "        for n in range(len(list_of_metrics)):\n",
        "            if i == 0:\n",
        "                y = hist[list_of_metrics[n]]\n",
        "                if n == 0:\n",
        "                    ax[i].plot(epochs, y, label=\"train\")\n",
        "                else:\n",
        "                    ax[i].plot(epochs, y, label=\"val\")\n",
        "                ax[i].set_title('Loss')\n",
        "                ax[i].legend(loc='upper right')\n",
        "                if n == 1:\n",
        "                    break\n",
        "            else:\n",
        "                if n >= 2:\n",
        "                    y = hist[list_of_metrics[n]]\n",
        "                    if n == 2:\n",
        "                        ax[i].plot(epochs, y, label=\"train\")\n",
        "                    else:\n",
        "                        ax[i].plot(epochs, y, label=\"val\")\n",
        "                    ax[i].set_title('Accuracy')\n",
        "                    ax[i].legend(loc='lower right')\n",
        "                    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "63qWXOa5uzA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curve(history.epoch, history.history, ['loss', 'val_loss'])"
      ],
      "metadata": {
        "id": "1E5zCBhru6iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그래프를 보니 train은 학습하면 할 수록 과적합이 되서 그런지 점점 loss가 줄어드는 모습을 보입니다. 그리고 validation에서의 loss는 train의 학습으로는 한계가 있는지 점점 loss가 줄어드는 폭이 좁아지는 것을 볼 수 있었습니다."
      ],
      "metadata": {
        "id": "lSmH5g9IvQu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "더 좋은 결과를 얻기 위해 어떤 과정이 남아 있을까?\n",
        "\n",
        "    lstm의 하이퍼 파라미터를 수정 또는 층을 늘린다.\n",
        "    epochs에 earlystopping을 추가하여 가장 강력할 때 멈춘다.\n",
        "    각 lstm 층마다 과적합 방지 기법을 사용한다. ( drop out, batchnormalization)\n",
        "    cross validation을 사용한다.\n",
        "    optimizer를 Adam말고 시퀀스 데이터에 더 적합하다고 알려진 rmsprop를 사용한다.\n"
      ],
      "metadata": {
        "id": "kp_7pSK4vYB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 회고"
      ],
      "metadata": {
        "id": "D83tA9zBuilT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- lstm을 진행하면서 lstm을 3층으로도 쌓고 dence를 2단으로 쌓아 더 강력하게하고 정규화 기법인 drop out이나 batchnormalization 등을 사용하여 단순하게 lstm을 돌렸을 때와 비교를 해보고 싶었지만 코드를 만들어본 결과 하나하나의 epochs가 시간이 너무 오래걸리기도 하고 많은 파라미터를 처리해야 하기 때문에 메모리가 부족할 것 같아서 진행을 못한 점이 아쉽습니다. \n",
        "- 어쨋든 간단한 lstm으로 간단한 데이터를 인풋으로 했을 때 적절한 답변을 얻는 모습을 볼 수 있었습니다. \n",
        "- 학습시간만 조금 짧았더라면 많은 시도를 했었을 것 같은데 아쉽네요!"
      ],
      "metadata": {
        "id": "KaFmnrQPulgd"
      }
    }
  ]
}