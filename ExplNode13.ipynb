{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twelvesense/first-repository/blob/master/ExplNode13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13-2. KorQuAD Task"
      ],
      "metadata": {
        "id": "rkziADVNLi1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# colab_dir = '/content/drive/MyDrive/colab/Expl13'"
      ],
      "metadata": {
        "id": "VIM46QKANOrF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_addons\n",
        "# !pip install sentencepiece "
      ],
      "metadata": {
        "id": "PlT2fO0ULwPF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# imports\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import collections\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import sentencepiece as spm\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "random_seed = 1234\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)"
      ],
      "outputs": [],
      "metadata": {
        "id": "8AlIpyOOLi1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# 다운로드한 KorQuAD 데이터를 확인해 봅시다. 아래 print_json_tree() 메서드는 KorQuAD 데이터처럼 json 포맷으로 이루어진 데이터에서 리스트의 첫 번째 아이템의 실제 내용을 간단히 확인하는데 유용합니다.\n",
        "\n",
        "def print_json_tree(data, indent=\"\"):\n",
        "    for key, value in data.items():\n",
        "        if type(value) == list:     # list 형태의 item은 첫번째 item만 출력\n",
        "            print(f'{indent}- {key}: [{len(value)}]')\n",
        "            print_json_tree(value[0], indent + \"  \")\n",
        "        else:\n",
        "            print(f'{indent}- {key}: {value}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "AorAaxw6Li1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# \n",
        "data_dir = './data/data'\n",
        "model_dir = './data/models'\n",
        "\n",
        "# 훈련데이터 확인\n",
        "train_json_path = data_dir + '/KorQuAD_v1.0_train.json'\n",
        "with open(train_json_path) as f:\n",
        "    train_json = json.load(f)\n",
        "    print_json_tree(train_json)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d4ab83291e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 훈련데이터 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_json_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/KorQuAD_v1.0_train.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_json_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint_json_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/data/KorQuAD_v1.0_train.json'"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "V9aieMgqLi1S",
        "outputId": "1933f8b4-3e5d-4e47-fd13-06eb291a5308"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 검증데이터 확인\n",
        "dev_json_path = data_dir + '/KorQuAD_v1.0_dev.json'\n",
        "with open(dev_json_path) as f:\n",
        "    dev_json = json.load(f)\n",
        "    print_json_tree(dev_json)"
      ],
      "outputs": [],
      "metadata": {
        "id": "09vbKGRVLi1T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# json 데이터의 실제 형태는 아래와 같이 json.dumps()를 이용해 확인해 볼 수 있습니다.\n",
        "print(json.dumps(train_json[\"data\"][0], indent=2, ensure_ascii=False))"
      ],
      "outputs": [],
      "metadata": {
        "id": "4vTGzIWHLi1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (1) 띄어쓰기 단위 정보관리"
      ],
      "metadata": {
        "id": "FEj3MiCWLi1V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# SQuAD, KorQuAD 데이터셋으로 모델을 구성하기 위한 전처리 과정은 다른 자연어처리 태스크와 다소 다른 접근법이 있습니다. 설명하기 다소 어려운 점이 있어서 코드를 실행하면서 예시를 들겠습니다.\n",
        "def _is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False"
      ],
      "outputs": [],
      "metadata": {
        "id": "q33EgfTgLi1V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# whitespace가 2개인 경우를 처리해야 함\n",
        "\n",
        "string1 = '1839년 파우스트를 읽었다.'\n",
        "string2 = '1839년  파우스트를 읽었다.'\n",
        "string1[6:10], string2[7:11]"
      ],
      "outputs": [],
      "metadata": {
        "id": "-7zy6LUrLi1X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 위 두 문장에 대해 글자별로 띄어쓰기 영역 정보를 관리해 주려면 다음과 같이 약간 다르게 처리될 것입니다.\n",
        "word_tokens = []\n",
        "char_to_word = []\n",
        "prev_is_whitespace = True\n",
        "\n",
        "# 첫번째 문장(string1)에 대해 띄어쓰기 영역 정보를 표시\n",
        "for c in string1:\n",
        "    if _is_whitespace(c):\n",
        "        prev_is_whitespace = True\n",
        "    else:\n",
        "        if prev_is_whitespace:\n",
        "            word_tokens.append(c)\n",
        "        else:\n",
        "            word_tokens[-1] += c\n",
        "        prev_is_whitespace = False    \n",
        "    char_to_word.append(len(word_tokens) - 1)\n",
        "    print(f'\\'{c}\\' : {word_tokens} : {char_to_word}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "LHqgEuv1Li1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#\n",
        "word_tokens = []\n",
        "char_to_word = []\n",
        "prev_is_whitespace = True\n",
        "\n",
        "# 두번째 문장(string2)에 대해 띄어쓰기 영역 정보를 표시\n",
        "for c in string2:\n",
        "    if _is_whitespace(c):\n",
        "        prev_is_whitespace = True\n",
        "    else:\n",
        "        if prev_is_whitespace:\n",
        "            word_tokens.append(c)\n",
        "        else:\n",
        "            word_tokens[-1] += c\n",
        "        prev_is_whitespace = False    \n",
        "    char_to_word.append(len(word_tokens) - 1)\n",
        "    print(f'\\'{c}\\' : {word_tokens} : {char_to_word}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "m409Bt0GLi1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "'''\n",
        "같은 코드이지만, '1839년' 다음의 공백 길이에 따라 두 문장의 영역 표시 결과가 조금 달라지는 것을 확인하셨나요?\n",
        "'''"
      ],
      "outputs": [],
      "metadata": {
        "id": "LIUVXcRnLi1Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 위에서 본 기능을 함수로 만들어 두면 다음과 같습니다.\n",
        "def _tokenize_whitespace(string):\n",
        "    word_tokens = []\n",
        "    char_to_word = []\n",
        "    prev_is_whitespace = True\n",
        "\n",
        "    for c in string:\n",
        "        if _is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                word_tokens.append(c)\n",
        "            else:\n",
        "                word_tokens[-1] += c\n",
        "            prev_is_whitespace = False    \n",
        "        char_to_word.append(len(word_tokens) - 1)\n",
        "    \n",
        "    return word_tokens, char_to_word"
      ],
      "outputs": [],
      "metadata": {
        "id": "TF9PKaL4Li1Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "'''\n",
        "위와 같은 방법으로 띄어쓰기 단위로 token을 정리한 후, word token 영역별로 유니크한 숫자(어절 번호)를 부여합니다. SQuAD 유형의 문제를 풀 때 글자 혹은 subword 단위로 token이 분리되는 것에 대비해서 원래 데이터가 띄어쓰기 단위로 어떠했었는지 word token 영역별로 추가 정보를 관리하면 도움이 됩니다. 아래와 같이 글자별로 word_token 영역을 표시해 주는 char_to_word list를 관리해 둡니다. 이 값은 현재 글자가 몇 번째 어절에 포함된 것이었는지를 말해 줍니다.\n",
        "'''"
      ],
      "outputs": [],
      "metadata": {
        "id": "6peHPrtDLi1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 위 두 문장에 대해 방금 만든 함수를 다시 적용해 보았습니다.\n",
        "# 첫번째 문장(string1)에 대해 띄어쓰기 영역 정보를 표시\n",
        "word_tokens, char_to_word = _tokenize_whitespace(string1)\n",
        "for c, i in zip(list(string1), char_to_word):\n",
        "    print(f'\\'{c}\\' : {i}')\n",
        "\n",
        "word_tokens, char_to_word"
      ],
      "outputs": [],
      "metadata": {
        "id": "i8b8OJXkLi1c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 두번째 문장(string2)에 대해 띄어쓰기 영역 정보를 표시\n",
        "word_tokens, char_to_word = _tokenize_whitespace(string2)\n",
        "for c, i in zip(list(string2), char_to_word):\n",
        "    print(f'\\'{c}\\' : {i}')\n",
        "\n",
        "word_tokens, char_to_word"
      ],
      "outputs": [],
      "metadata": {
        "id": "zfCcqjMaLi1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (2) Tokenize by Vocab\n",
        "읽다, 읽었다, 읽어라, 읽고, 읽으려면, 읽다가....\n",
        "이 모든 단어를 전부 단어사전에 추가하려면 너무 많은 단어가 필요하겠죠? Word 기반의 단어사전 구축이 가지는 문제점입니다. 특히 한국어의 경우에는 이런 문제점이 심각하겠죠? 만약 '읽었다'를 '읽'+'었다' 로 나누어서 처리할 수 있다면 어떨까요?\n",
        "이런 접근법을 'Subword Segmentation'이라고 합니다.\n",
        "BERT에는 WordPiece 모델 사용이 일반적이지만, 오늘 우리는 SentencePiece 모델을 이용해서 Subword 기반의 텍스트 전처리를 진행할 것입니다. 구글에서 오픈소스로 제공하는 SentencePiece 모델은 파이썬에서 손쉽게 사용 가능하며, WordPiece 등 다른 모델들을 통합하여 제공하므로 최근 널리 사용되고 있습니다."
      ],
      "metadata": {
        "id": "kFq_Yb_vLi1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 다만, 한국어의 경우에는 koNLPy를 통해 사용할 수 있는 형태소 분석기가 이런 역할을 합니다. 하지만 SentencePiece 같은 모델들은 언어마다 다른 문법 규칙을 활용하지 않고, 적절한 Subword 분절 규칙을 학습하거나, 혹은 자주 사용되는 구문을 하나의 단어로 묶어내는 등 통계적인 방법을 사용합니다. 그래서 어떤 언어에든 보편적으로 적용 가능하다는 장점이 있습니다.\n",
        "\n",
        "# vocab loading\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(f\"{model_dir}/ko_32000.model\")\n",
        "\n",
        "# word를 subword로 변경하면서 index 저장\n",
        "word_to_token = []\n",
        "context_tokens = []\n",
        "for (i, word) in enumerate(word_tokens):\n",
        "    word_to_token.append(len(context_tokens))\n",
        "    tokens = vocab.encode_as_pieces(word)  # SentencePiece를 사용해 Subword로 쪼갭니다.\n",
        "    for token in tokens:\n",
        "        context_tokens.append(token)\n",
        "\n",
        "context_tokens, word_to_token"
      ],
      "outputs": [],
      "metadata": {
        "id": "CNe7_6ZrLi1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "'''\n",
        "    위에서 '_읽'에는 '_'이 있고, '었다'에는 '_'가 없는 것이 눈에 띄시나요? '_' 표시는 앞부분이 공백이라는 뜻입니다.\n",
        "\n",
        "여기서 word_to_token의 [0, 2, 5]란 context_tokens에 쪼개져 담긴 0번, 2번, 5번 토큰인 '▁1839', '▁', '▁읽' 이 어절 단위의 첫 번째 토큰이 된다는 정보를 담아둔 것입니다.\n",
        "'''"
      ],
      "outputs": [],
      "metadata": {
        "id": "n4bWRL_lLi1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 그러면 SentencePiece를 활용하는 위 코드도 아래와 같이 함수로 만들어 두면 유용할 것입니다.\n",
        "def _tokenize_vocab(vocab, context_words):\n",
        "    word_to_token = []\n",
        "    context_tokens = []\n",
        "    for (i, word) in enumerate(context_words):\n",
        "        word_to_token.append(len(context_tokens))\n",
        "        tokens = vocab.encode_as_pieces(word)\n",
        "        for token in tokens:\n",
        "            context_tokens.append(token)\n",
        "    return context_tokens, word_to_token"
      ],
      "outputs": [],
      "metadata": {
        "id": "O81j9TWJLi1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "print(word_tokens)  # 처리해야 할 word 단위 입력\n",
        "\n",
        "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
        "context_tokens, word_to_token   # Subword 단위로 토큰화한 결과"
      ],
      "outputs": [],
      "metadata": {
        "id": "-Jvy1pyKLi1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (3) Improve Span"
      ],
      "metadata": {
        "id": "-oqrOOWeLi1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# KorQuAD 데이터셋에서 context, question, answer를 뽑아 봅니다. KorQuAD 데이터셋은 질문(question)과 지문(context)을 주고, 지문 영역에서 정답(answer)을 찾도록 구성되어 있습니다. 그러므로 정답에 해당하는 지문 영역을 정확히 찾아내는 것이 전처리의 핵심적인 작업이 됩니다.\n",
        "\n",
        "context = train_json['data'][0]['paragraphs'][0]['context']\n",
        "question = train_json['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
        "answer_text = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n",
        "answer_start = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
        "answer_end = answer_start + len(answer_text) - 1\n",
        "\n",
        "print('[context] ', context)\n",
        "print('[question] ', question)\n",
        "print('[answer] ', answer_text)\n",
        "print('[answer_start] index: ', answer_start, 'character: ', context[answer_start])\n",
        "print('[answer_end]index: ', answer_end, 'character: ', context[answer_end])\n",
        "\n",
        "# answer_text에 해당하는 context 영역을 정확히 찾아내야 합니다. \n",
        "assert context[answer_start:answer_end + 1] == answer_text"
      ],
      "outputs": [],
      "metadata": {
        "id": "Oo9to_eaLi1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# context를 띄어쓰기(word) 단위로 토큰화한 결과를 살펴봅니다. \n",
        "word_tokens, char_to_word = _tokenize_whitespace(context)\n",
        "\n",
        "print( word_tokens[:20])\n",
        "\n",
        "char_to_word[:20], context[:20]"
      ],
      "outputs": [],
      "metadata": {
        "id": "1SdPayxCLi1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 띄어쓰기(word) 단위로 쪼개진 context(word_tokens)를 Subword로 토큰화한 결과를 살펴봅니다. \n",
        "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
        "for i in range(min(20, len(word_to_token) - 1)):\n",
        "    print(word_to_token[i], context_tokens[word_to_token[i]:word_to_token[i + 1]])"
      ],
      "outputs": [],
      "metadata": {
        "id": "kgDcFP2_Li1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 이제 질문의 답을 떠올려 봅시다. 위에서 우리는 context에 포함된 answer의 글자 단위 시작 인덱스 answer_start와 종료 인덱스 answer_end를 구했습니다. 이 위치를 어절(word) 단위로 변환하면 어떻게 될까요?\n",
        "# answer_start와 answer_end로부터 word_start와 word_end를 구합니다. \n",
        "word_start = char_to_word[answer_start]\n",
        "word_end = char_to_word[answer_end]\n",
        "word_start, word_end, answer_text, word_tokens[word_start:word_end + 1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "CKMk7f9RLi1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*우리가 찾는 정답은 15번째 어절(index=14)에 있었군요. 하지만 우리가 원하는 정답은 '교향곡'이지, '교향곡을'은 아닙니다."
      ],
      "metadata": {
        "id": "3gxxv1NdLi1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 그래서 이번에는 word_start로부터 word_end까지의 context를 Subword 단위로 토큰화한 결과를 살펴봅시다.\n",
        "token_start = word_to_token[word_start]\n",
        "if word_end < len(word_to_token) - 1:\n",
        "    token_end = word_to_token[word_end + 1] - 1\n",
        "else:\n",
        "    token_end = len(context_tokens) - 1\n",
        "token_start, token_end, context_tokens[token_start:token_end + 1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "_zokSW42Li1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 이제 거의 정답에 근접했습니다. Subword 단위로 토큰화한 결과 중에는 우리가 찾는 정답과 정확히 일치하는 답이 있는 것 같습니다.\n",
        "# 실제 정답인 answer_text도 Subword 기준으로 토큰화해 둡니다. \n",
        "token_answer = \" \".join(vocab.encode_as_pieces(answer_text))\n",
        "token_answer"
      ],
      "outputs": [],
      "metadata": {
        "id": "hE6zibysLi1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이제 눈으로 봐도 어디가 정확히 정답인지 알 수 있게 되었지만, 좀 더 일반적인 방법으로 정답 토큰 범위를 찾는 코드를 작성해 보겠습니다. KorQuAD 문제의 정답은 이번처럼 단답형만 있는 것은 아니기 때문입니다."
      ],
      "metadata": {
        "id": "iuzv12sULi1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 정답이 될수 있는 new_start와 new_end의 경우를 순회탐색합니다. \n",
        "for new_start in range(token_start, token_end + 1):\n",
        "    for new_end in range(token_end, new_start - 1, -1):\n",
        "        text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
        "        if text_span == token_answer:   # 정답과 일치하는 경우\n",
        "            print(\"O >>\", (new_start, new_end), text_span)\n",
        "        else:\n",
        "            print(\"X >>\", (new_start, new_end), text_span)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VwrRfbTwLi1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 이제 context에서 answer의 위치를 토큰화된 상태에서 찾는 함수를 아래와 같이 정리할 수 있게 되었습니다.\n",
        "# context_tokens에서 char_answer의 위치를 찾아 리턴하는 함수\n",
        "def _improve_span(vocab, context_tokens, token_start, token_end, char_answer):\n",
        "    token_answer = \" \".join(vocab.encode_as_pieces(char_answer))\n",
        "    for new_start in range(token_start, token_end + 1):\n",
        "        for new_end in range(token_end, new_start - 1, -1):\n",
        "            text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
        "            if text_span == token_answer:\n",
        "                return (new_start, new_end)\n",
        "    return (token_start, token_end)"
      ],
      "outputs": [],
      "metadata": {
        "id": "fv3fz7auLi1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 잘 작동하는지 확인해 봅시다.\n",
        "token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, answer_text)\n",
        "print('token_start:', token_start, ' token_end:', token_end)\n",
        "context_tokens[token_start:token_end + 1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "g43CzhixLi1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (4) 데이터셋 분리"
      ],
      "metadata": {
        "id": "mrD_jNAMLi1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# train 데이터셋, dev 데이터셋을 분리하여, 위에서 작성한 _improve_span() 함수를 이용해 전처리 후 파일로 저장합니다.\n",
        "def dump_korquad(vocab, json_data, out_file):\n",
        "    with open(out_file, \"w\",  encoding='utf8') as f:\n",
        "        for data in tqdm(json_data[\"data\"]):\n",
        "            title = data[\"title\"]\n",
        "            for paragraph in data[\"paragraphs\"]:\n",
        "                context = paragraph[\"context\"]\n",
        "                context_words, char_to_word = _tokenize_whitespace(context)\n",
        "\n",
        "                for qa in paragraph[\"qas\"]:\n",
        "                    assert len(qa[\"answers\"]) == 1\n",
        "                    qa_id = qa[\"id\"]\n",
        "                    question = qa[\"question\"]\n",
        "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                    answer_start = qa[\"answers\"][0][\"answer_start\"]\n",
        "                    answer_end = answer_start + len(answer_text) - 1\n",
        "\n",
        "                    assert answer_text == context[answer_start:answer_end + 1]\n",
        "\n",
        "                    word_start = char_to_word[answer_start]\n",
        "                    word_end = char_to_word[answer_end]\n",
        "\n",
        "                    word_answer = \" \".join(context_words[word_start:word_end + 1])\n",
        "                    char_answer = \" \".join(answer_text.strip().split())\n",
        "                    assert char_answer in word_answer\n",
        "\n",
        "                    context_tokens, word_to_token = _tokenize_vocab(vocab, context_words)\n",
        "\n",
        "                    token_start = word_to_token[word_start]\n",
        "                    if word_end < len(word_to_token) - 1:\n",
        "                        token_end = word_to_token[word_end + 1] - 1\n",
        "                    else:\n",
        "                        token_end = len(context_tokens) - 1\n",
        "\n",
        "                    token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, char_answer)\n",
        "\n",
        "                    data = {\"qa_id\": qa_id, \"title\": title, \"question\": vocab.encode_as_pieces(question), \"context\": context_tokens, \"answer\": char_answer, \"token_start\": token_start, \"token_end\":token_end}\n",
        "                    f.write(json.dumps(data, ensure_ascii=False))\n",
        "                    f.write(\"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "MUPmiqCmLi1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PASS"
      ],
      "metadata": {
        "id": "-VmB9m2ULi1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 전처리를 수행하여 파일로 생성합니다. \n",
        "# dump_korquad(vocab, train_json, f\"{data_dir}/korquad_train.json\")\n",
        "# dump_korquad(vocab, dev_json, f\"{data_dir}/korquad_dev.json\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "O2BniMwtLi1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 전처리가 의도대로 잘 되었는지 실제로 파일 내용을 확인해 볼까요?\n",
        "\n",
        "def print_file(filename, count=10):\n",
        "    \"\"\"\n",
        "    파일 내용 출력\n",
        "    :param filename: 파일 이름\n",
        "    :param count: 출력 라인 수\n",
        "    \"\"\"\n",
        "    with open(filename, encoding='utf8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if count <= i:\n",
        "                break\n",
        "            print(line.strip())\n",
        "\n",
        "print_file(f\"{data_dir}/korquad_train.json\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "CgNVdRMaLi1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (5) 데이터 분석 : Question\n",
        "원본 데이터셋을 전처리하여 우리의 모델이 다루게 될 데이터셋으로 가공하는 과정을 진행하였습니다.\n",
        "그러나 이 데이터셋을 그대로 사용할 수 있을지, 혹은 이상(abnormal) 데이터가 존재하지는 않는지 분석하는 과정이 필요합니다."
      ],
      "metadata": {
        "id": "ocAoS6IkLi1j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 우선 전체 데이터에서 question 항목의 길이 분포를 조사해 보겠습니다.\n",
        "questions = []\n",
        "contexts = []\n",
        "token_starts = []\n",
        "with open(f\"{data_dir}/korquad_train.json\", encoding='utf8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        data = json.loads(line)\n",
        "        questions.append(data[\"question\"])\n",
        "        contexts.append(data[\"context\"])\n",
        "        token_starts.append(data[\"token_start\"])\n",
        "        if i < 10:\n",
        "            print(data[\"token_start\"], data[\"question\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "PrDmE7pTLi1j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# token count\n",
        "train_question_counts = [len(question) for question in questions]\n",
        "train_question_counts[:10]"
      ],
      "outputs": [],
      "metadata": {
        "id": "4lZPleYfLi1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 그래프에 대한 이미지 사이즈 선언\n",
        "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
        "plt.figure(figsize=(8, 4))\n",
        "# histogram 선언\n",
        "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
        "# range: x축 값의 범위\n",
        "# facecolor: 그래프 색상\n",
        "# label: 그래프에 대한 라벨\n",
        "plt.hist(train_question_counts, bins=100, range=[0, 100], facecolor='b', label='train')\n",
        "# 그래프 제목\n",
        "plt.title('Count of question')\n",
        "# 그래프 x 축 라벨\n",
        "plt.xlabel('Number of question')\n",
        "# 그래프 y 축 라벨\n",
        "plt.ylabel('Count of question')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "mc9pp7p2Li1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 데이터 길이\n",
        "print(f\"question 길이 최대:    {np.max(train_question_counts):4d}\")\n",
        "print(f\"question 길이 최소:    {np.min(train_question_counts):4d}\")\n",
        "print(f\"question 길이 평균:    {np.mean(train_question_counts):7.2f}\")\n",
        "print(f\"question 길이 표준편차: {np.std(train_question_counts):7.2f}\")\n",
        "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
        "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
        "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
        "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
        "percentile25 = np.percentile(train_question_counts, 25)\n",
        "percentile50 = np.percentile(train_question_counts, 50)\n",
        "percentile75 = np.percentile(train_question_counts, 75)\n",
        "percentileIQR = percentile75 - percentile25\n",
        "percentileMAX = percentile75 + percentileIQR * 1.5\n",
        "print(f\"question 25/100분위:  {percentile25:7.2f}\")\n",
        "print(f\"question 50/100분위:  {percentile50:7.2f}\")\n",
        "print(f\"question 75/100분위:  {percentile75:7.2f}\")\n",
        "print(f\"question IQR:        {percentileIQR:7.2f}\")\n",
        "print(f\"question MAX/100분위: {percentileMAX:7.2f}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "AsjTVfqZLi1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "plt.figure(figsize=(4, 6))\n",
        "# 박스플롯 생성\n",
        "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
        "# labels: 입력한 데이터에 대한 라벨\n",
        "# showmeans: 평균값을 표현\n",
        "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
        "plt.boxplot(train_question_counts, labels=['token counts'], showmeans=True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "RHX4LlPlLi1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (6) 데이터 분석 : Context"
      ],
      "metadata": {
        "id": "XF6SVDZoLi1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 위와 동일한 방법으로 context 항목에 대해서도 분석해 봅니다.\n",
        "# token count\n",
        "train_context_counts = [len(context) for context in contexts]\n",
        "train_context_counts[:10]"
      ],
      "outputs": [],
      "metadata": {
        "id": "LjR6F20RLi1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 그래프에 대한 이미지 사이즈 선언\n",
        "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
        "plt.figure(figsize=(8, 4))\n",
        "# histogram 선언\n",
        "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
        "# range: x축 값의 범위\n",
        "# facecolor: 그래프 색상\n",
        "# label: 그래프에 대한 라벨\n",
        "plt.hist(train_context_counts, bins=900, range=[100, 1000], facecolor='r', label='train')\n",
        "# 그래프 제목\n",
        "plt.title('Count of context')\n",
        "# 그래프 x 축 라벨\n",
        "plt.xlabel('Number of context')\n",
        "# 그래프 y 축 라벨\n",
        "plt.ylabel('Count of context')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "w2HM_Ur8Li1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 데이터 길이\n",
        "print(f\"context 길이 최대:    {np.max(train_context_counts):4d}\")\n",
        "print(f\"context 길이 최소:    {np.min(train_context_counts):4d}\")\n",
        "print(f\"context 길이 평균:    {np.mean(train_context_counts):7.2f}\")\n",
        "print(f\"context 길이 표준편차: {np.std(train_context_counts):7.2f}\")\n",
        "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
        "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
        "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
        "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
        "percentile25 = np.percentile(train_context_counts, 25)\n",
        "percentile50 = np.percentile(train_context_counts, 50)\n",
        "percentile75 = np.percentile(train_context_counts, 75)\n",
        "percentileIQR = percentile75 - percentile25\n",
        "percentileMAX = percentile75 + percentileIQR * 1.5\n",
        "print(f\"context 25/100분위:  {percentile25:7.2f}\")\n",
        "print(f\"context 50/100분위:  {percentile50:7.2f}\")\n",
        "print(f\"context 75/100분위:  {percentile75:7.2f}\")\n",
        "print(f\"context IQR:        {percentileIQR:7.2f}\")\n",
        "print(f\"context MAX/100분위: {percentileMAX:7.2f}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "IJQq79oULi1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "plt.figure(figsize=(4, 6))\n",
        "# 박스플롯 생성\n",
        "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
        "# labels: 입력한 데이터에 대한 라벨\n",
        "# showmeans: 평균값을 표현\n",
        "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
        "plt.boxplot(train_context_counts, labels=['token counts'], showmeans=True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "73E-KzK4Li1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (7) 데이터 분석 : Answer"
      ],
      "metadata": {
        "id": "9ZuYbr5bLi1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 위와 동일한 방법으로 answer 항목에 대해서도 분석해 봅니다.\n",
        "# token count\n",
        "train_answer_starts = token_starts\n",
        "train_answer_starts[:10]"
      ],
      "outputs": [],
      "metadata": {
        "id": "cD1VrnpLLi1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 그래프에 대한 이미지 사이즈 선언\n",
        "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
        "plt.figure(figsize=(8, 4))\n",
        "# histogram 선언\n",
        "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
        "# range: x축 값의 범위\n",
        "# facecolor: 그래프 색상\n",
        "# label: 그래프에 대한 라벨\n",
        "plt.hist(train_answer_starts, bins=500, range=[0, 500], facecolor='g', label='train')\n",
        "# 그래프 제목\n",
        "plt.title('Count of answer')\n",
        "# 그래프 x 축 라벨\n",
        "plt.xlabel('Number of answer')\n",
        "# 그래프 y 축 라벨\n",
        "plt.ylabel('Count of answer')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ix-qyCbHLi1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 데이터 길이\n",
        "print(f\"answer 위치 최대:    {np.max(train_answer_starts):4d}\")\n",
        "print(f\"answer 위치 최소:    {np.min(train_answer_starts):4d}\")\n",
        "print(f\"answer 위치 평균:    {np.mean(train_answer_starts):7.2f}\")\n",
        "print(f\"answer 위치 표준편차: {np.std(train_answer_starts):7.2f}\")\n",
        "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
        "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
        "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
        "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
        "percentile25 = np.percentile(train_answer_starts, 25)\n",
        "percentile50 = np.percentile(train_answer_starts, 50)\n",
        "percentile75 = np.percentile(train_answer_starts, 75)\n",
        "percentileIQR = percentile75 - percentile25\n",
        "percentileMAX = percentile75 + percentileIQR * 1.5\n",
        "print(f\"answer 25/100분위:  {percentile25:7.2f}\")\n",
        "print(f\"answer 50/100분위:  {percentile50:7.2f}\")\n",
        "print(f\"answer 75/100분위:  {percentile75:7.2f}\")\n",
        "print(f\"answer IQR:        {percentileIQR:7.2f}\")\n",
        "print(f\"answer MAX/100분위: {percentileMAX:7.2f}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "cqcVujD0Li1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "plt.figure(figsize=(4, 6))\n",
        "# 박스플롯 생성\n",
        "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
        "# labels: 입력한 데이터에 대한 라벨\n",
        "# showmeans: 평균값을 표현\n",
        "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
        "plt.boxplot(train_answer_starts, labels=['token counts'], showmeans=True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "vQxBeJviLi1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (8) 데이터 분석 : Word Cloud"
      ],
      "metadata": {
        "id": "p5B2qD4PLi1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 워드 클라우드(Word Cloud)란 자료의 빈도수를 시각화해서 나타내는 방법입니다. 문서의 핵심 단어를 한눈에 파악할 수 있고, 빅데이터를 분석할 때 데이터의 특징을 도출하기 위해서 활용됩니다. 빈도수가 높은 단어일수록 글씨 크기가 큰 특징이 있습니다. 아래 코드를 실행시켜 워드 클라우드를 확인해 봅시다.\n",
        "# train documents\n",
        "documents = []\n",
        "\n",
        "# 전체 데이터에서 title, context, question 문장을 모두 추출합니다. \n",
        "for data in tqdm(train_json[\"data\"]):\n",
        "    title = data[\"title\"]\n",
        "    documents.append(title)\n",
        "    for paragraph in data[\"paragraphs\"]:\n",
        "        context = paragraph[\"context\"]\n",
        "        documents.append(context)\n",
        "\n",
        "        for qa in paragraph[\"qas\"]:\n",
        "            assert len(qa[\"answers\"]) == 1\n",
        "            question = qa[\"question\"]\n",
        "            documents.append(question)\n",
        "\n",
        "documents[:10]   # 그중 맨 앞 10개만 확인해 봅니다."
      ],
      "outputs": [],
      "metadata": {
        "id": "j-cAPXN7Li1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# documents를 전부 이어 하나의 문장으로 만들면 이렇게 보입니다. \n",
        "\" \".join(documents[:10])"
      ],
      "outputs": [],
      "metadata": {
        "id": "EGjpUY_lLi1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# WordCloud로 \" \".join(documents)를 처리해 봅니다. \n",
        "# wordcloud = WordCloud(width=800, height=800, font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(\" \".join(documents))\n",
        "wordcloud = WordCloud(width=800, height=800, font_path='./data/NanumGothic.ttf').generate(\" \".join(documents))\n",
        "plt.figure(figsize=(10, 10))\n",
        "# image 출력, interpolation 이미지 시각화 옵션\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "uSeeuDUTLi1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋 전처리 (9) 데이터 로드"
      ],
      "metadata": {
        "id": "GDQiLwfWLi1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 지금까지 만든 데이터셋을 메모리에 로드합니다.\n",
        "train_json = os.path.join(data_dir, \"korquad_train.json\")\n",
        "dev_json = os.path.join(data_dir, \"korquad_dev.json\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "7t-M9jLNLi1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "class Config(dict):\n",
        "    \"\"\"\n",
        "    json을 config 형태로 사용하기 위한 Class\n",
        "    :param dict: config dictionary\n",
        "    \"\"\"\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "\n",
        "args = Config({\n",
        "    'max_seq_length': 384,\n",
        "    'max_query_length': 64,\n",
        "})\n",
        "args"
      ],
      "outputs": [],
      "metadata": {
        "id": "V_11jrfWLi1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 생성한 데이터셋 파일을 메모리에 로딩하는 함수\n",
        "def load_data(args, filename):\n",
        "    inputs, segments, labels_start, labels_end = [], [], [], []\n",
        "\n",
        "    n_discard = 0\n",
        "    with open(filename, \"r\", encoding='utf8') as f:\n",
        "        for i, line in enumerate(tqdm(f, desc=f\"Loading ...\")):\n",
        "            data = json.loads(line)\n",
        "            token_start = data.get(\"token_start\")\n",
        "            token_end = data.get(\"token_end\")\n",
        "            question = data[\"question\"][:args.max_query_length]\n",
        "            context = data[\"context\"]\n",
        "            answer_tokens = \" \".join(context[token_start:token_end + 1])\n",
        "            context_len = args.max_seq_length - len(question) - 3\n",
        "\n",
        "            if token_end >= context_len:\n",
        "                # 최대 길이내에 token이 들어가지 않은 경우 처리하지 않음\n",
        "                n_discard += 1\n",
        "                continue\n",
        "            context = context[:context_len]\n",
        "            assert len(question) + len(context) <= args.max_seq_length - 3\n",
        "\n",
        "            tokens = ['[CLS]'] + question + ['[SEP]'] + context + ['[SEP]']\n",
        "            ids = [vocab.piece_to_id(token) for token in tokens]\n",
        "            ids += [0] * (args.max_seq_length - len(ids))\n",
        "            inputs.append(ids)\n",
        "            segs = [0] * (len(question) + 2) + [1] * (len(context) + 1)\n",
        "            segs += [0] * (args.max_seq_length - len(segs))\n",
        "            segments.append(segs)\n",
        "            token_start += (len(question) + 2)\n",
        "            labels_start.append(token_start)\n",
        "            token_end += (len(question) + 2)\n",
        "            labels_end.append(token_end)\n",
        "    print(f'n_discard: {n_discard}')\n",
        "\n",
        "    return (np.array(inputs), np.array(segments)), (np.array(labels_start), np.array(labels_end))"
      ],
      "outputs": [],
      "metadata": {
        "id": "EE-yNWUALi1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# train data load\n",
        "train_inputs, train_labels = load_data(args, train_json)\n",
        "print(f\"train_inputs: {train_inputs[0].shape}\")\n",
        "print(f\"train_inputs: {train_inputs[1].shape}\")\n",
        "print(f\"train_labels: {train_labels[0].shape}\")\n",
        "print(f\"train_labels: {train_labels[1].shape}\")\n",
        "\n",
        "# dev data load\n",
        "dev_inputs, dev_labels = load_data(args, dev_json)\n",
        "print(f\"dev_inputs: {dev_inputs[0].shape}\")\n",
        "print(f\"dev_inputs: {dev_inputs[1].shape}\")\n",
        "print(f\"dev_labels: {dev_labels[0].shape}\")\n",
        "print(f\"dev_labels: {dev_labels[1].shape}\")\n",
        "\n",
        "train_inputs[:10], train_labels[:10]"
      ],
      "outputs": [],
      "metadata": {
        "id": "xPLMh1o0Li1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 우리가 만든 데이터셋은 최종적으로 이렇게 생겼습니다.\n",
        "# Question과 Context가 포함된 입력데이터 1번째\n",
        "train_inputs[0][0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "prUkPxn-Li1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Question을 0으로, Context를 1로 구분해 준 Segment 데이터 1번째\n",
        "train_inputs[1][0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "jbxUuN0uLi1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Answer위치의 시작점과 끝점 라벨 1번째\n",
        "train_labels[0][0], train_labels[1][0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "FLTGfrRBLi1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13-3. LSTM을 이용한 도전"
      ],
      "metadata": {
        "id": "W0H6i6GpLi1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 우선 KorQuAD 태스크를 LSTM 모델을 활용하여 학습해 봅시다.\n",
        "'''\n",
        "다소 복잡해 보이겠지만 Input이 2개, Output이 2개인 모델이라는 점에 주목해 주십시오. 2개의 Input은 이전 스텝에서 보았던 train_inputs[0], train_inputs[1]이 들어갑니다. 이들은 각각 Question+Context의 데이터와 Segment입니다. 그리고 Output은 Answer의 시작점과 끝점의 위치입니다.\n",
        "'''\n",
        "def build_model_lstm(n_vocab, n_seq, d_model):\n",
        "    tokens = tf.keras.layers.Input((None,), name='tokens')\n",
        "    segments = tf.keras.layers.Input((None,), name='segments')\n",
        "\n",
        "    hidden = tf.keras.layers.Embedding(n_vocab, d_model)(tokens) + tf.keras.layers.Embedding(2, d_model)(segments) # (bs, n_seq, d_model)\n",
        "\n",
        "    hidden = tf.keras.layers.LSTM(d_model, return_sequences=True)(hidden)  # (bs, n_seq, d_model)\n",
        "    hidden = tf.keras.layers.LSTM(d_model, return_sequences=True)(hidden)  # (bs, n_seq, d_model)\n",
        "    hidden = tf.keras.layers.Dense(2)(hidden) # (bs, n_seq, 2)\n",
        "    start_logits, end_logits = tf.split(hidden, 2, axis=-1)  # (bs, n_seq, 1), (bs, n_seq, 1)\n",
        "    start_logits = tf.squeeze(start_logits, axis=-1)  # (bs, n_seq)\n",
        "    start_outputs = tf.keras.layers.Softmax(name=\"start\")(start_logits)\n",
        "    end_logits = tf.squeeze(end_logits, axis=-1)  # (bs, n_seq)\n",
        "    end_outputs = tf.keras.layers.Softmax(name=\"end\")(end_logits)\n",
        "\n",
        "    model = tf.keras.Model(inputs=(tokens, segments), outputs=(start_outputs, end_outputs))\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "0I273QLULi1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "model = build_model_lstm(n_vocab=len(vocab), n_seq=512, d_model=512)\n",
        "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xJ5AiNPNLi1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), metrics=[\"accuracy\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "8qo2vlWDLi1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 훈련을 진행해 봅시다. 대략 1 epoch에 2~3분가량 소요됩니다.\n",
        "# 시간상 6 epochs만 진행해 봅시다. 2 epochs 이상 val_start_accuracy가 좋아지지 않으면 훈련을 종료하도록 Early Stopping을 적용합니다.\n",
        "# early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_start_accuracy', patience=2)\n",
        "# save weights\n",
        "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(data_dir, \"korquad_lstm.hdf5\"), monitor='val_start_accuracy', verbose=1, save_best_only=True, mode='max', save_freq='epoch', save_weights_only=True)\n",
        "\n",
        "history = model.fit(train_inputs, train_labels, epochs=6, batch_size=128, validation_data=(dev_inputs, dev_labels), callbacks=[early_stopping, save_weights])"
      ],
      "outputs": [],
      "metadata": {
        "id": "R9Z2BkSALi1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 훈련이 마무리되었으면 시각화를 진행해 봅시다.\n",
        "# training result\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['start_accuracy'], 'g-', label='start_accuracy')\n",
        "plt.plot(history.history['val_start_accuracy'], 'k--', label='val_start_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['end_accuracy'], 'b-', label='end_accuracy')\n",
        "plt.plot(history.history['val_end_accuracy'], 'g--', label='val_end_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "lLWsIfg5Li1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM을 통해 진행했던 결과가 어떤가요? 아마도 val_loss가 낮아지지 않고, val_accuracy들도 크게 좋아지지 않는 것을 보실 수 있습니다.\n",
        "KorQuAD 태스크는 데이터셋만 가지고 사전 준비 없이 학습했을 때 일정 이상 좋아지지 않는다는 것을 알 수 있습니다. 모델을 다양하게 바꾸어 보아도 결과는 비슷할 것입니다.\n",
        "그렇다면 어떻게 해야 이 태스크를 학습할 수 있을까요?"
      ],
      "metadata": {
        "id": "jbHG8o6tLi1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13-4. BERT의 모델 구조"
      ],
      "metadata": {
        "id": "Tt92BRyBLi1w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 그러면 이제 실제 코드를 통해 BERT 모델 구성을 더욱 디테일하게 살펴보겠습니다.\n",
        "\n",
        "# 유틸리티 함수들\n",
        "\n",
        "def get_pad_mask(tokens, i_pad=0):\n",
        "    \"\"\"\n",
        "    pad mask 계산하는 함수\n",
        "    :param tokens: tokens (bs, n_seq)\n",
        "    :param i_pad: id of pad\n",
        "    :return mask: pad mask (pad: 1, other: 0)\n",
        "    \"\"\"\n",
        "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
        "    mask = tf.expand_dims(mask, axis=1)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def get_ahead_mask(tokens, i_pad=0):\n",
        "    \"\"\"\n",
        "    ahead mask 계산하는 함수\n",
        "    :param tokens: tokens (bs, n_seq)\n",
        "    :param i_pad: id of pad\n",
        "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
        "    \"\"\"\n",
        "    n_seq = tf.shape(tokens)[1]\n",
        "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
        "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
        "    pad_mask = get_pad_mask(tokens, i_pad)\n",
        "    mask = tf.maximum(ahead_mask, pad_mask)\n",
        "    return mask\n",
        "\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    gelu activation 함수\n",
        "    :param x: 입력 값\n",
        "    :return: gelu activation result\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n",
        "\n",
        "\n",
        "def kernel_initializer(stddev=0.02):\n",
        "    \"\"\"\n",
        "    parameter initializer 생성\n",
        "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
        "    \"\"\"\n",
        "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
        "\n",
        "\n",
        "def bias_initializer():\n",
        "    \"\"\"\n",
        "    bias initializer 생성\n",
        "    \"\"\"\n",
        "    return tf.zeros_initializer\n",
        "\n",
        "\n",
        "class Config(dict):\n",
        "    \"\"\"\n",
        "    json을 config 형태로 사용하기 위한 Class\n",
        "    :param dict: config dictionary\n",
        "    \"\"\"\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        \"\"\"\n",
        "        file에서 Config를 생성 함\n",
        "        :param file: filename\n",
        "        \"\"\"\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0Z9Ceds_Li1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# mode == \"embedding\" 일 경우 Token Embedding Layer 로 사용되는 layer 클래스입니다. \n",
        "\n",
        "class SharedEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Weighed Shared Embedding Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.n_vocab = config.n_vocab\n",
        "        self.d_model = config.d_model\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        shared weight 생성\n",
        "        :param input_shape: Tensor Shape (not used)\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"shared_embedding_weight\"):\n",
        "            self.shared_weights = self.add_weight(\n",
        "                \"weights\",\n",
        "                shape=[self.n_vocab, self.d_model],\n",
        "                initializer=kernel_initializer()\n",
        "            )\n",
        "\n",
        "    def call(self, inputs, mode=\"embedding\"):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: 입력\n",
        "        :param mode: 실행 모드\n",
        "        :return: embedding or linear 실행 결과\n",
        "        \"\"\"\n",
        "        # mode가 embedding일 경우 embedding lookup 실행\n",
        "        if mode == \"embedding\":\n",
        "            return self._embedding(inputs)\n",
        "        # mode가 linear일 경우 linear 실행\n",
        "        elif mode == \"linear\":\n",
        "            return self._linear(inputs)\n",
        "        # mode가 기타일 경우 오류 발생\n",
        "        else:\n",
        "            raise ValueError(f\"mode {mode} is not valid.\")\n",
        "    \n",
        "    def _embedding(self, inputs):\n",
        "        \"\"\"\n",
        "        embedding lookup\n",
        "        :param inputs: 입력\n",
        "        \"\"\"\n",
        "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
        "        return embed\n",
        "\n",
        "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
        "        \"\"\"\n",
        "        linear 실행\n",
        "        :param inputs: 입력\n",
        "        \"\"\"\n",
        "        n_batch = tf.shape(inputs)[0]\n",
        "        n_seq = tf.shape(inputs)[1]\n",
        "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
        "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
        "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
        "        return outputs"
      ],
      "outputs": [],
      "metadata": {
        "id": "WzOh7u0LLi1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Positional Embedding Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"position_embedding\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: 입력\n",
        "        :return embed: positional embedding lookup 결과\n",
        "        \"\"\"\n",
        "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
        "        embed = self.embedding(position)\n",
        "        return embed"
      ],
      "outputs": [],
      "metadata": {
        "id": "WY6IfoJ0Li1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
        "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Scale Dot Product Attention Class\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param Q: Q value\n",
        "        :param K: K value\n",
        "        :param V: V value\n",
        "        :param attn_mask: 실행 모드\n",
        "        :return attn_out: attention 실행 결과\n",
        "        \"\"\"\n",
        "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
        "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
        "        attn_scale = tf.math.divide(attn_score, scale)\n",
        "        attn_scale -= 1.e9 * attn_mask\n",
        "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
        "        attn_out = tf.matmul(attn_prob, V)\n",
        "        return attn_out"
      ],
      "outputs": [],
      "metadata": {
        "id": "jTdIXZo1Li1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi Head Attention Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"multi_head_attention\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.d_model = config.d_model\n",
        "        self.n_head = config.n_head\n",
        "        self.d_head = config.d_head\n",
        "\n",
        "        # Q, K, V input dense layer\n",
        "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        # Scale Dot Product Attention class\n",
        "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
        "        # output dense layer\n",
        "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "\n",
        "    def call(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param Q: Q value\n",
        "        :param K: K value\n",
        "        :param V: V value\n",
        "        :param attn_mask: 실행 모드\n",
        "        :return attn_out: attention 실행 결과\n",
        "        \"\"\"\n",
        "        # reshape Q, K, V, attn_mask\n",
        "        batch_size = tf.shape(Q)[0]\n",
        "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
        "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
        "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
        "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
        "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
        "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
        "        # transpose and liner\n",
        "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
        "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
        "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
        "\n",
        "        return attn_out"
      ],
      "outputs": [],
      "metadata": {
        "id": "gHK9Pww5Li1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
        "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Position Wise Feed Forward Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"feed_forward\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: inputs\n",
        "        :return ff_val: feed forward 실행 결과\n",
        "        \"\"\"\n",
        "        ff_val = self.W_2(self.W_1(inputs))\n",
        "        return ff_val"
      ],
      "outputs": [],
      "metadata": {
        "id": "SJaoSXggLi1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# class EncoderLayer(tf.keras.layers.Layer):\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Encoder Layer Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"encoder_layer\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(config)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForward(config)\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
        " \n",
        "    def call(self, enc_embed, self_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
        "        :param self_mask: enc_tokens의 pad mask\n",
        "        :return enc_out: EncoderLayer 실행 결과\n",
        "        \"\"\"\n",
        "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
        "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
        "\n",
        "        ffn_val = self.ffn(norm1_val)\n",
        "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
        "\n",
        "        return enc_out"
      ],
      "outputs": [],
      "metadata": {
        "id": "7SsGPBWbLi1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 위와 같이 BERT를 구성하는 레이어들이 준비되었습니다. 아래 BERT 모델 구현을 통해 위에서 설명했던 레이어들이 어떻게 서로 결합되어 있는지 살펴보시기 바랍니다.\n",
        "class BERT(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    BERT Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"bert\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.i_pad = config.i_pad\n",
        "        self.embedding = SharedEmbedding(config)\n",
        "        self.position = PositionalEmbedding(config)\n",
        "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
        "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "        \n",
        "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
        "\n",
        "    def call(self, enc_tokens, segments):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param enc_tokens: encoder tokens\n",
        "        :param segments: token segments\n",
        "        :return logits_cls: CLS 결과 logits\n",
        "        :return logits_lm: LM 결과 logits\n",
        "        \"\"\"\n",
        "        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)\n",
        "\n",
        "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
        "\n",
        "        enc_out = self.dropout(enc_embed)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
        "\n",
        "        logits_cls = enc_out[:,0]\n",
        "        logits_lm = enc_out\n",
        "        return logits_cls, logits_lm\n",
        "    \n",
        "    def get_embedding(self, tokens, segments):\n",
        "        \"\"\"\n",
        "        token embedding, position embedding lookup\n",
        "        :param tokens: 입력 tokens\n",
        "        :param segments: 입력 segments\n",
        "        :return embed: embedding 결과\n",
        "        \"\"\"\n",
        "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
        "        embed = self.norm(embed)\n",
        "        return embed"
      ],
      "outputs": [],
      "metadata": {
        "id": "xZ65a6XVLi1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13-5. BERT 모델을 이용한 도전"
      ],
      "metadata": {
        "id": "vayMcmfzLi1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 이제 BERT 모델을 활용하여, LSTM으로 풀어보았던 KorQuAD 태스크를 다시 학습해 봅시다. 아마도 데이터셋 구성은 동일하기 때문에 별도의 추가 조치가 필요 없을 것입니다. 모델의 차이만 비교해 보기 위해 일부러 두 모델이 사용하는 Tokenizer를 동일하게 구성하였습니다.\n",
        "# 아래는 BERT 레이어에 Fully Connected layer를 붙어 KorQuAD용으로 finetune하기 위한 모델 클래스입니다.\n",
        "class BERT4KorQuAD(tf.keras.Model):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(name='BERT4KorQuAD')\n",
        "\n",
        "        self.bert = BERT(config)\n",
        "        self.dense = tf.keras.layers.Dense(2)\n",
        "    \n",
        "    def call(self, enc_tokens, segments):\n",
        "        logits_cls, logits_lm = self.bert(enc_tokens, segments)\n",
        "\n",
        "        hidden = self.dense(logits_lm) # (bs, n_seq, 2)\n",
        "        start_logits, end_logits = tf.split(hidden, 2, axis=-1)  # (bs, n_seq, 1), (bs, n_seq, 1)\n",
        "\n",
        "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
        "        start_outputs = tf.keras.layers.Softmax(name=\"start\")(start_logits)\n",
        "\n",
        "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
        "        end_outputs = tf.keras.layers.Softmax(name=\"end\")(end_logits)\n",
        "\n",
        "        return start_outputs, end_outputs"
      ],
      "outputs": [],
      "metadata": {
        "id": "3NaTKJO5Li1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "config = Config({\"d_model\": 512, \"n_head\": 8, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 6, \"n_seq\": 384, \"n_vocab\": 0, \"i_pad\": 0})\n",
        "config.n_vocab = len(vocab)\n",
        "config.i_pad = vocab.pad_id()\n",
        "config"
      ],
      "outputs": [],
      "metadata": {
        "id": "_9YGiurgLi1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 모델의 크기가 다르고, 사용할 수 있는 배치 사이즈가 달라지므로, 배치 구성만 다시 진행하겠습니다.\n",
        "bert_batch_size = 32 \n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)).shuffle(10000).batch(bert_batch_size)\n",
        "dev_dataset = tf.data.Dataset.from_tensor_slices((dev_inputs, dev_labels)).batch(bert_batch_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "XAWZfnomLi1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \n",
        "model = BERT4KorQuAD(config)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-dsCEJ4nLi10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 이번에는 BERT 모델만 구성한 후 전혀 pretraining 없이 학습을 진행해 보겠습니다. 과연 결과가 어떨까요? (pretrained model을 활용하는 것은 프로젝트 스텝에서 진행할 예정입니다.)\n",
        "def train_epoch(model, dataset, loss_fn, acc_fn, optimizer):\n",
        "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
        "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
        "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
        "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
        "\n",
        "    p_bar = tqdm(dataset)\n",
        "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(p_bar):\n",
        "        with tf.GradientTape() as tape:\n",
        "            start_outputs, end_outputs = model(enc_tokens, segments)\n",
        "\n",
        "            start_loss = loss_fn(start_labels, start_outputs)\n",
        "            end_loss = loss_fn(end_labels, end_outputs)\n",
        "            loss = start_loss + end_loss\n",
        "\n",
        "            start_acc = acc_fn(start_labels, start_outputs)\n",
        "            end_acc = acc_fn(end_labels, end_outputs)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        metric_start_loss(start_loss)\n",
        "        metric_end_loss(end_loss)\n",
        "        metric_start_acc(start_acc)\n",
        "        metric_end_acc(end_acc)\n",
        "        if batch % 10 == 9:\n",
        "            p_bar.set_description(f'loss: {metric_start_loss.result():0.4f}, {metric_end_loss.result():0.4f}, acc: {metric_start_acc.result():0.4f}, {metric_end_acc.result():0.4f}')\n",
        "    p_bar.close()\n",
        "\n",
        "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
      ],
      "outputs": [],
      "metadata": {
        "id": "m9jNgt9ZLi10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# def eval_epoch(model, dataset, loss_fn, acc_fn):\n",
        "def eval_epoch(model, dataset, loss_fn, acc_fn):\n",
        "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
        "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
        "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
        "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
        "\n",
        "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(dataset):\n",
        "        start_outputs, end_outputs = model(enc_tokens, segments)\n",
        "\n",
        "        start_loss = loss_fn(start_labels, start_outputs)\n",
        "        end_loss = loss_fn(end_labels, end_outputs)\n",
        "\n",
        "        start_acc = acc_fn(start_labels, start_outputs)\n",
        "        end_acc = acc_fn(end_labels, end_outputs)\n",
        "\n",
        "        metric_start_loss(start_loss)\n",
        "        metric_end_loss(end_loss)\n",
        "        metric_start_acc(start_acc)\n",
        "        metric_end_acc(end_acc)\n",
        "\n",
        "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "#                                  OOM occured                                 #\n",
        "# ---------------------------------------------------------------------------- #"
      ],
      "outputs": [],
      "metadata": {
        "id": "83qzCHg4Li10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OOM occured\n",
        " ---------------------------------------------------------------------------- #\n",
        "                                  OOM occured                                 #\n",
        " ---------------------------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "Q7cLndv2Li11"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ExplNode13.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  }
}